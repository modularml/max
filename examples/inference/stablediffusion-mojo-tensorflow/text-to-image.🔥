# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #


## Global Mojo imports
import sys
import random

from os import setenv

from collections import List
from math import exp, log, sin, cos, sqrt
from memory import memcpy
from python import Python
from tensor import randn, Tensor, TensorShape, TensorSpec

from max.engine import InferenceSession, Model, SessionOptions

## Local Mojo imports
import python_utils

## Typedefs
alias uint8 = DType.uint8
alias int32 = DType.int32
alias int64 = DType.int64
alias float = DType.float32

alias GUIDANCE_SCALE_FACTOR = 7.5


fn tokenize(
    np: PythonObject, tokenizer: PythonObject, prompt: String, N: Int
) raises -> Tensor[int32]:
    """Invoke the python tokenizer and convert the result to a mojo Tensor."""
    var inputs = tokenizer.tokenize(prompt, N)
    var tokens = python_utils.numpy_to_tensor[int64](inputs).astype[int32]()
    tokens.ireshape(TensorShape(1, tokens.num_elements()))
    return tokens


fn arange[dtype: DType](start: Int, stop: Int, step: Int) -> Tensor[dtype]:
    """Return a sequence vector from <start> to <stop>, incrementing by <step>.
    """
    var N = len(range(start, stop, step))
    var x = Tensor[dtype](TensorSpec(dtype, N))
    for i in range(N):
        x[i] = range(start, stop, step)[i]
    return x


## Mojo doesn't yet support slice notation like x[::-1], so we need a reverse()
## helper function instead.
fn reverse[dtype: DType](x: Tensor[dtype]) -> Tensor[dtype]:
    """Return a tensor with the elements of x in reverse order; i.e., x[::-1].
    """
    var N = x.num_elements()
    var xout = Tensor[dtype](x.spec())
    for i in range(N):
        xout[i] = x[N - i - 1]
    return xout


fn vstack[dtype: DType](a: Tensor[dtype], b: Tensor[dtype]) -> Tensor[dtype]:
    """Concatenate tensors a & b along the outermost dimension."""
    # Generate return shape
    var out_shape = List[Int]()
    out_shape.append(a.shape()[0] + b.shape()[0])
    for i in range(1, a.shape().rank()):
        out_shape.append(a.shape()[i])
    # Allocate return tensor
    var out = Tensor[dtype](TensorShape(out_shape))
    # Fill data
    memcpy(out.data(), a.data(), a.num_elements())
    memcpy(out.data() + a.num_elements(), b.data(), b.num_elements())
    return out


fn split[dtype: DType](x: Tensor[dtype], i: Int) -> Tensor[dtype]:
    """Return the ith slice of the outermost dim; i.e., x[idx, :, :, ..., :]."""
    # Generate return shape
    var shape = List[Int]()
    shape.append(1)
    for i in range(1, x.shape().rank()):
        shape.append(x.shape()[i])
    # Allocate return tensor
    var ret = Tensor[dtype](TensorShape(shape))
    # Fill data
    memcpy(
        ret.data(),
        x.data() + (x.num_elements() // 2) * i,
        x.num_elements() // 2,
    )
    return ret


fn get_timestep_embeddings(t_step: Int, num_channels: Int) -> Tensor[float]:
    """Generate 1D embedding vector for given timestep & num-channels."""
    var c_by_2 = num_channels // 2
    # Generate sequence vector
    var vseq = arange[float](0, c_by_2, 1) / Float32(c_by_2)
    # Allocate output tensor
    var temb = Tensor[float](TensorSpec(float, num_channels))

    # Generate frequencies & fill lower/upper half with cos/sin
    for i in range(c_by_2):
        var freq = exp(-log[float, 1](10000) * vseq[i]) * t_step
        temb[i] = cos(freq)
        temb[c_by_2 + i] = sin(freq)
    return temb


## The MAX execute APIs index input & output tensors by name rather than
## position. But the TF models use ugly output names and all of our models
## are single-output, so rather than polluting main() with lots of calls
## like `var x = res.get[float]("my-ugly-tensor-name")` we define wrappers
## here that extract the only output and return it directly.
fn execute(
    model: Model, name0: String, val0: Tensor[float]
) raises -> Tensor[float]:
    """Run the given single-input single-output model and return the result."""
    return model.execute(name0, val0).get[float](
        model.get_model_output_names()[0]
    )


fn execute(
    model: Model,
    name0: String,
    val0: Tensor[int32],
    name1: String,
    val1: Tensor[int32],
) raises -> Tensor[float]:
    """Run the dual-input single-output model and return the result."""
    return model.execute(name0, val0, name1, val1).get[float](
        model.get_model_output_names()[0]
    )


fn execute(
    model: Model,
    name0: String,
    val0: Tensor[float],
    name1: String,
    val1: Tensor[float],
    name2: String,
    val2: Tensor[float],
) raises -> Tensor[float]:
    """Run the triple-input single-output model and return the result."""
    return model.execute(name0, val0, name1, val1, name2, val2).get[float](
        model.get_model_output_names()[0]
    )


fn main() raises -> None:
    ## Parse args
    var USAGE = "Usage: text-to-image.ðŸ”¥ --prompt <str> " + "[--negative-prompt <str>] [--num-steps <int>] [--seed <int>] [-o <str>]"

    var argv = sys.argv()
    if len(argv) % 2 == 0:
        print(USAGE)
        raise Error("All options require an argument")

    # Suppress extraneous logging
    _ = setenv("TF_CPP_MIN_LOG_LEVEL", "3")
    _ = setenv("TRANSFORMERS_VERBOSITY", "critical")
    _ = setenv("TOKENIZERS_PARALLELISM", "false")

    # Set default values
    var prompt: String = ""
    var negative_prompt: String = ""
    var num_steps: Int = 25
    var seed: Int = 0
    var model_dir: String = "../../models/stable-diffusion-tensorflow"
    var output: String = "output.png"

    for i in range(1, len(argv), 2):
        if argv[i] == "--prompt":
            prompt = argv[i + 1]
        elif argv[i] == "--negative-prompt":
            negative_prompt = argv[i + 1]
        elif argv[i] == "--num-steps":
            num_steps = atol(argv[i + 1])
        elif argv[i] == "--seed":
            seed = atol(argv[i + 1])
        elif argv[i] == "--model-dir":
            model_dir = argv[i + 1]
        elif argv[i] == "-o" or argv[i] == "--output":
            output = argv[i + 1]
        else:
            print(USAGE)
            raise Error("Unknown option")

    # Only required arg is --prompt
    if prompt == "":
        print(USAGE)
        raise Error("--prompt option is required")

    random.seed(seed)

    ## Import python modules
    Python.add_to_path(".")
    var np = Python.import_module("numpy")
    var Image = Python.import_module("PIL.Image")
    var Tokenizer = Python.import_module("tokenizer")
    var ALPHAS = Python.import_module("constants")._ALPHAS_CUMPROD

    ## Compile & load models
    print("Loading models into MAX AI Engine...")
    var session = InferenceSession()
    var txt_encoder = session.load_model(model_dir + "/txt-encoder")
    var img_decoder = session.load_model(model_dir + "/img-decoder")
    var img_diffuser = session.load_model(model_dir + "/img-diffuser")

    ## Tokenize & encode prompts
    print("Encoding inputs...")
    var N = 77  # Max sequence length of text encoder
    var tokenizer = Tokenizer.SimpleTokenizer()
    var prompt_p = tokenize(np, tokenizer, prompt, N)
    var prompt_n = tokenize(np, tokenizer, negative_prompt, N)

    # Generate position ids & reshape to (1,N)
    var positions = arange[int32](0, N, 1)
    positions.ireshape(TensorShape(1, N))

    # TODO: Merge these into a single batch
    var context_p = execute(
        txt_encoder, "tokens", prompt_p, "positions", positions
    )
    var context_n = execute(
        txt_encoder, "tokens", prompt_n, "positions", positions
    )
    var context = vstack(context_p, context_n)

    ## Initialize latent, timestep and alpha inputs.
    print("Initializing latent...")
    # From model inspection, we know that h=w=64 and c=4
    # Note: For tensorflow, shapes are given in NHWC format.
    var latent = randn[float](TensorShape(1, 64, 64, 4))
    var timesteps = reverse(arange[int32](1, 1000, 1000 // num_steps))
    var num_timesteps = timesteps.num_elements()

    # We import alphas from constants.py to avoid forking the 'header' file.
    # This means we need an extra call to to_float64() to convert into mojo.
    var alphas = Tensor[float](TensorShape(num_timesteps + 1))
    for i in range(num_timesteps):
        alphas[i] = ALPHAS[timesteps[i]].to_float64().cast[float]()
    alphas[num_timesteps] = 1.0

    ## Loop through diffusion model
    for i in range(num_timesteps):
        print("\rGenerating image: ", i + 1, "/", num_timesteps, end="")
        var latent_prev = latent

        # Generate embeddings for ith timestep & reshape to 1xC
        # From model inspection, we know that num_embeddings_channels=320
        var temb = get_timestep_embeddings(int(timesteps[i]), 320)
        temb.ireshape(TensorShape(1, 320))

        # Execute diffusion model
        var new_latent = execute(
            img_diffuser,
            "latent",
            vstack(latent, latent),
            "context",
            context,
            "timestep_embedding",
            vstack(temb, temb),
        )

        # Merge conditioned & unconditioned latents
        var latent0 = split[float](new_latent, 0)
        var latent1 = split[float](new_latent, 1)
        latent = latent1 + (latent0 - latent1) * GUIDANCE_SCALE_FACTOR

        # Merge latent with previous iteration
        var pred = (latent_prev - latent * sqrt(1 - alphas[i])) / sqrt(
            alphas[i]
        )
        latent = latent * sqrt(1 - alphas[i + 1]) + pred * sqrt(alphas[i + 1])
    print("")

    # Decode final latent
    print("Decoding image...")
    var decoded = execute(img_decoder, "input_2", latent)
    var pixels = (((decoded + 1.0) / 2.0).clip(0, 1) * 255.0).astype[uint8]()
    var np_pixels = python_utils.tensor_to_numpy[uint8](pixels, np)
    var img = Image.fromarray(np.squeeze(np_pixels), "RGB")
    _ = img.save(output)
    return
