{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP: Mistral-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install -q transformers optimum[onnxruntime]\n",
    "!python3 -m pip install -q torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the conversion once. It takes around 15min to complete on c5.12xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export onnx --model \"mistralai/Mistral-7B-v0.1\" \"./onnx/mistral-7b-onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': tensor([[    1,  4315,   863,   272, 13088,  3893,   272,  3878, 28804]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "options: TorchLoadOptions(input_specs=[<max.engine.api.TorchInputSpec object at 0x7f23559a5720>, <max.engine.api.TorchInputSpec object at 0x7f23555b97e0>], type='torch')\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from transformers import AutoTokenizer\n",
    "from max import engine\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "max_seq_len = 128\n",
    "INPUT = \"Why did the chicken cross the road?\"\n",
    "inputs = tokenizer(INPUT, return_tensors=\"pt\", return_token_type_ids=False, max_length=max_seq_len)\n",
    "print(f\"inputs: {inputs}\")\n",
    "#max_tokenizer = partial(tokenizer, return_tensors=\"pt\", padding=\"max_length\", max_length=max_seq_len)\n",
    "#inputs = max_tokenizer(text)\n",
    "input_spec_list = [\n",
    "    engine.TorchInputSpec(shape=tensor.size(), dtype=engine.DType.int64)\n",
    "    for tensor in inputs.values()\n",
    "]\n",
    "options = engine.TorchLoadOptions(input_spec_list)\n",
    "print(f\"options: {options}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First compilation takes around 10min on c5.12xlarge. Subsequent is faster ~ 2-5min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 s, sys: 29.9 s, total: 52.6 s\n",
      "Wall time: 2min 48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling model.    \n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session = engine.InferenceSession()\n",
    "model = session.load(\"./onnx/mistral-7b-onnx/model.onnx\", options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect\n",
    "# for tensor in model.input_metadata:\n",
    "#     print(f'name: {tensor.name}, shape: {tensor.shape}, dtype: {tensor.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs['position_ids'] = torch.arange(inputs['input_ids'].size(1), dtype=torch.long, device=inputs['input_ids'].device).unsqueeze(0)\n",
    "# TODO: remove the hardcoded 32\n",
    "for i in range(32):\n",
    "    inputs[f\"past_key_values.{i}.key\"] = torch.zeros([1,8,0,128], dtype=torch.float)\n",
    "    inputs[f\"past_key_values.{i}.value\"] = torch.zeros([1,8,0,128], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.52 s, sys: 44 ms, total: 7.56 s\n",
      "Wall time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "output = model.execute(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First iteration generated text: # do you chicken cross the road?\n",
      "\n",
      "CPU times: user 13.7 ms, sys: 0 ns, total: 13.7 ms\n",
      "Wall time: 1.33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logits = torch.tensor(output[\"logits\"], dtype=torch.float)\n",
    "generated_token_ids = torch.argmax(logits, dim=-1)\n",
    "generated_text = tokenizer.decode(generated_token_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "print(\"First iteration generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Generated text iter 0: #1 we cross cross the road?\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Generated text iter 1: #1? cross the the road?\n",
      "\n",
      "To\n",
      "================================================================================\n",
      "Generated text iter 2: #1:\n",
      " the road road?\n",
      "\n",
      "To get\n",
      "================================================================================\n",
      "Generated text iter 3: #1:\n",
      "\n",
      " road??\n",
      "\n",
      "To get to\n",
      "================================================================================\n",
      "Generated text iter 4: #1: To\n",
      "\n",
      "??\n",
      "\n",
      " get to the\n",
      "================================================================================\n",
      "Generated text iter 5: #1: To get\n",
      "\n",
      "?\n",
      "\n",
      " to the other\n",
      "================================================================================\n",
      "Generated text iter 6: #1: To get to\n",
      "\n",
      "\n",
      "\n",
      "To the other side\n",
      "================================================================================\n",
      "Generated text iter 7: #1: To get to the\n",
      "\n",
      "\n",
      "To the other side.\n",
      "================================================================================\n",
      "Generated text iter 8: #1: To get to the road\n",
      "ToWhy the other side.\n",
      "\n",
      "================================================================================\n",
      "Generated text iter 9: #1: To get to the other?\n",
      " get did other side.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Generated text iter 10: #1: To get to the other side\n",
      "Why tod side.\n",
      "\n",
      "Why\n",
      "================================================================================\n",
      "Generated text iter 11: #1: To get to the other side.Why didays.\n",
      "\n",
      "\n",
      " did\n",
      "================================================================================\n",
      "Generated text iter 12: #1: To get to the other side.\n",
      " did theide\n",
      "\n",
      "\n",
      "Why the\n",
      "================================================================================\n",
      "Generated text iter 13: #1: To get to the other side.\n",
      "\n",
      " the other.\n",
      "WhyWhy did chicken\n",
      "================================================================================\n",
      "Generated text iter 14: #1: To get to the other side.\n",
      "\n",
      "\n",
      " other side\n",
      "\n",
      " did did the #\n",
      "================================================================================\n",
      "Generated text iter 15: #1: To get to the other side.\n",
      "\n",
      "\n",
      "\n",
      " side.\n",
      "Why the the chicken2\n",
      "================================================================================\n",
      "Generated text iter 16: #1:\n",
      " get to the other side.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Why did chicken chicken cross:\n",
      "================================================================================\n",
      "Generated text iter 17: #1:\n",
      "\n",
      " to the other side.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Why did the\n",
      " cross the\n",
      "\n",
      "================================================================================\n",
      "Generated text iter 18: #2:\n",
      "\n",
      "\n",
      " the other side.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WhyWhy did the chickencross the roadroad\n",
      "================================================================================\n",
      "Generated text iter 19: #2:\n",
      "\n",
      "\n",
      "\n",
      " other side.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Why did did the chicken\n",
      "ounter the road??\n"
     ]
    }
   ],
   "source": [
    "# TODO: cleanup after examination\n",
    "for j in range(20):\n",
    "    inputs = tokenizer(generated_text, return_tensors=\"pt\", return_token_type_ids=False)\n",
    "    inputs['position_ids'] = torch.arange(inputs['input_ids'].size(1), dtype=torch.long, device=inputs['input_ids'].device).unsqueeze(0)\n",
    "    for i in range(32):\n",
    "        inputs[f\"past_key_values.{i}.key\"] = torch.tensor(output[f\"present.{i}.key\"], dtype=torch.float)\n",
    "        inputs[f\"past_key_values.{i}.value\"] = torch.tensor(output[f\"present.{i}.value\"], dtype=torch.float)\n",
    "\n",
    "    inputs[\"attention_mask\"] = torch.ones([1, inputs[f\"past_key_values.0.key\"].size(2) + inputs[\"input_ids\"].size(1)], dtype=torch.int64)\n",
    "    # print(inputs[\"attention_mask\"].shape)\n",
    "    output = model.execute(**inputs)\n",
    "    logits = torch.tensor(output[\"logits\"], dtype=torch.float)\n",
    "    generated_token_ids = torch.argmax(logits, dim=-1)\n",
    "    generated_text = tokenizer.decode(generated_token_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"Generated text iter {j}: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
