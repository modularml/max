{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright 2024 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python API for MAX Engine enables you to upgrade your runtime performance for TensorFlow and PyTorch models, on a wide range of hardware, with just three lines of code (not counting the import):\n",
    "\n",
    "\n",
    "```python\n",
    "from max import engine\n",
    "\n",
    "# Load your model:\n",
    "session = engine.InferenceSession()\n",
    "model = session.load(model_path)\n",
    "\n",
    "# Prepare the inputs, then run an inference:\n",
    "outputs = model.execute(**inputs)\n",
    "\n",
    "# Process the output here.\n",
    "```\n",
    "\n",
    "That's all you need! Everything else is the usual code to prepare your\n",
    "inputs and process the outputs.\n",
    "\n",
    "But, it's also nice to see a fully working example. So the\n",
    "rest of this page shows how to run an inference using a version of\n",
    "[RoBERTa from Cardiff\n",
    "NLP](https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion-multilabel-latest),\n",
    "which is a language model trained on tweets to perform sentiment analysis.\n",
    "\n",
    "This example uses is a TensorFlow\n",
    "model (which must be converted to SavedModel format), and it's just as easy to\n",
    "load a model from PyTorch (which must be converted to TorchScript format).\n",
    "\n",
    ":::note Try it\n",
    "\n",
    "This page is written in a Jupyter notebook you can get from\n",
    "[our GitHub repo](https://github.com/modularml/max/blob/main/examples/notebooks/roberta-python-tensorflow.ipynb).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the MAX Engine Python package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, you first need to install the `max` Python package.\n",
    "This package is not hosted in a package repository (PyPI), and can only be\n",
    "installed with the `modular` CLI tool.\n",
    "\n",
    "For instructions, see\n",
    "[Get started with MAX Engine](https://docs.modular.com/engine/get-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the MAX Engine Python package\n",
    "!python3 -m pip install -q --find-links \"$(modular config max.path)/wheels\" max-engine\n",
    "# Install other packages\n",
    "# If you're using Python 3.9+, please run the following.\n",
    "!python3 -m pip install -q transformers tensorflow-cpu\n",
    "# If you're running Python 3.8, please run this instead.\n",
    "!python3 -m pip install -q transformers tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start coding, we need some libraries that help us get the model and process\n",
    "the input/output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "# suppress extraneous logging\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"critical\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/max/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "from max import engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download the [RoBERTa model](https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion-multilabel-latest) from HuggingFace and save it in TensorFlow SavedModel format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: roberta/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: roberta/assets\n"
     ]
    }
   ],
   "source": [
    "HF_MODEL_NAME = \"cardiffnlp/twitter-roberta-base-emotion-multilabel-latest\"\n",
    "hf_model = TFAutoModelForSequenceClassification.from_pretrained(HF_MODEL_NAME)\n",
    "\n",
    "model_path = Path(\"roberta\")\n",
    "tf.saved_model.save(hf_model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load and compile the model in MAX Engine using an\n",
    "[`InferenceSession`](/engine/reference/python/engine.html#max.engine.InferenceSession)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling model.    \n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "session = engine.InferenceSession()\n",
    "model = session.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's two lines down, just one to go.\n",
    "\n",
    ":::note\n",
    "\n",
    "Some models might take a few minutes to compile the first time, but this up-front cost will pay in dividends with latency savings provided by our next-generation graph compiler.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is your usual pre-processing. \n",
    "For the RoBERTa model, we need to process the text input into a sequence of tokens, so we'll do that with [`transformers.AutoTokenizer`](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer).\n",
    "\n",
    "First, let's take a look at the model's inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: attention_mask, shape: [None, None], dtype: DType.int32\n",
      "name: input_ids, shape: [None, None], dtype: DType.int32\n",
      "name: token_type_ids, shape: [None, None], dtype: DType.int32\n"
     ]
    }
   ],
   "source": [
    "for tensor in model.input_metadata:\n",
    "    print(f'name: {tensor.name}, shape: {tensor.shape}, dtype: {tensor.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us the model needs 3 inputs. When a dimension size is `None`, that means it's dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([[    0,   970,    32,   171,  3571,  5126,    11,     5,   882,\n",
      "            9,  4687, 13469,   328,     2]]), 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "INPUT=\"There are many exciting developments in the field of AI Infrastructure!\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
    "inputs = tokenizer(INPUT, return_tensors=\"np\", return_token_type_ids=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for that third line of code, we pass the inputs to\n",
    "[`execute()`](/engine/reference/python/engine#max.engine.Model.execute). This\n",
    "function requires all inputs as keyword arguments, so we'll\n",
    "unpack the `inputs` dictionary as we pass it through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logits': array([[-4.7728553 ,  0.67335933, -4.7689624 , -0.78787935,  3.842757  ,\n",
      "        -2.0537782 ,  2.1849515 , -4.227579  , -5.3352814 , -1.3866923 ,\n",
      "        -1.0697807 ]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "outputs = model.execute(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!\n",
    "\n",
    "The output from [`execute()`](/engine/reference/python/engine.html#max.engine.Model.execute) is a dictionary of output tensors, each in an `ndarray`. Let's now figure out what they say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll use some help from the [transformers library](https://huggingface.co/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification) to convert the output ids to labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is: joy\n"
     ]
    }
   ],
   "source": [
    "# Extract class prediction from output\n",
    "predicted_class_id = outputs[\"logits\"].argmax(axis=-1)[0]\n",
    "classification = hf_model.config.id2label[predicted_class_id]\n",
    "\n",
    "print(f\"The sentiment is: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta-da! 🎉\n",
    "\n",
    "If you're running this notebook yourself, beware that this notebook does not illustrate MAX Engine's runtime performance. For actual benchmark results, try [our benchmark tool](/engine/benchmark) or check out our [performance dashboard](https://performance.modular.com).\n",
    "\n",
    "For more details about the inferencing API, see the [Python API reference](/engine/reference/python/engine)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
