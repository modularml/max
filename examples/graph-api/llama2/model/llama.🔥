# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

from collections import List, Optional
from pathlib import Path

from max.graph import ops, Dim, Graph, TensorType, Type, Symbol

from weights.loadable_model import LoadableModel, LlamaHParams


def rope(x: Symbol, freqs_cis: Symbol) -> Symbol:
    """Applies rotary positional embeddings (RoPE) to `x`.

    Args:
        x: Activation tensor with shape (batch, seq_len, n_kv_heads, head_dim).
        freqs_cis: Positional frequencies tensor with shape
            (seq_len, head_dim // 2, 2).

    Returns:
        Input activation tensor with rotary positional embeddings applied and
        the same shape as `x`.
    """
    var x_complex = ops.as_interleaved_complex(x)
    var x_dims = x_complex.type().tensor().dims

    var freqs_cis_bcast = ops.unsqueeze(ops.unsqueeze(freqs_cis, 1), 0)

    var x_re = x_complex[0, axis= -1]
    var x_im = x_complex[1, axis= -1]

    var freqs_re = freqs_cis_bcast[0, axis= -1].rebind(
        1, x_dims[1], 1, x_dims[3]
    )
    var freqs_im = freqs_cis_bcast[1, axis= -1].rebind(
        1, x_dims[1], 1, x_dims[3]
    )

    var rope_re = (x_re * freqs_re) - (x_im * freqs_im)
    var rope_im = (x_re * freqs_im) + (x_im * freqs_re)
    var rope_complex = ops.as_complex(rope_re, rope_im)

    return ops.reshape_like(rope_complex, x)


def rope_custom_kernel(x: Symbol, freqs_cis: Symbol) -> Symbol:
    """Applies rotary positional embeddings (RoPE) to `x`.

    Args:
        x: Activation tensor with shape (batch, seq_len, n_kv_heads, head_dim).
        freqs_cis: Positional frequencies tensor with shape
            (seq_len, head_dim // 2, 2).

    Returns:
        Input activation tensor with rotary positional embeddings applied and
        the same shape as `x`.
    """
    # The kernel requires freqs_cis to be 2-D.
    # Reshape freqs (seq_len, head_dim // 2, 2) => (seq_len, head_dim).
    var f_shape = ops.shape_of(freqs_cis)
    var new_f_shape = ops.stack(
        List[Symbol](f_shape[0], x.graph().scalar(Int64(-1)))
    )
    var freqs_2d = ops.reshape(freqs_cis, new_f_shape)

    return ops.custom["rope"](List[Symbol](x, freqs_2d), x.tensor_type())


@value
struct Attention[float_dtype: DType = DType.float32]:
    var n_heads: Int
    var n_kv_heads: Int
    var head_dim: Int
    var dim: Int
    var n_rep: Int
    var enable_custom_rope_kernel: Bool

    var wq: Symbol
    var wk: Symbol
    var wv: Symbol
    var wo: Symbol

    def repeat_kv(self, v: Symbol, n_rep: Int) -> Symbol:
        if self.n_kv_heads < self.n_heads:
            raise "Not yet supported"
        return v

    def attention_mask(self, start_pos: Symbol, seq_len: Symbol) -> Symbol:
        var g = start_pos.graph()

        seq_len = seq_len.reshape()
        start_pos = start_pos.reshape()

        # Mask out current sequence elements [i, j] where j > i with an
        # upper-triangular matrix filled with -inf.
        var mask_val = g.full(
            Scalar[float_dtype].MIN, List[Symbol](seq_len, seq_len)
        )
        var mask = ops.band_part(
            mask_val,
            g.scalar[DType.int64](-1),
            num_upper=g.scalar[DType.int64](0),
            # Invert the mask from lower to upper.
            exclude=True,
        )

        # Compute attention scores only for the new sequence.
        # Hence for a matrix of scores of size (seqlen, cache_len + seqlen),
        # the only masked entries are (i, j) for j > cache_len + i, since row i
        # corresponds to token cache_len + i.
        return ops.concat(
            List[Symbol](
                g.full[DType.float32](0, List[Symbol](seq_len, start_pos)), mask
            ),
            axis=1,
        )

    def __call__(
        self,
        input: Symbol,
        start_pos: Symbol,
        freqs_cis: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> Tuple[Symbol, Symbol, Symbol]:
        var g = input.graph()
        var input_shape = ops.shape_of(input)

        var batch_size = input_shape[0]
        var seq_len = input_shape[1]
        var head_dim = g.scalar[float_dtype](self.head_dim)

        var xq = input @ self.wq
        var xk = input @ self.wk
        var xv = input @ self.wv

        xq = xq.reshape(batch_size, seq_len, self.n_heads, self.head_dim)
        xk = xk.reshape(batch_size, seq_len, self.n_kv_heads, self.head_dim)
        xv = xv.reshape(batch_size, seq_len, self.n_kv_heads, self.head_dim)

        xq = rope_custom_kernel(
            xq, freqs_cis
        ) if self.enable_custom_rope_kernel else rope(xq, freqs_cis)
        xk = rope_custom_kernel(
            xk, freqs_cis
        ) if self.enable_custom_rope_kernel else rope(xk, freqs_cis)

        var keys = ops.concat(
            List[Symbol](k_cache, xk.swapaxes(0, 1))
        ).swapaxes(0, 1)
        var values = ops.concat(
            List[Symbol](v_cache, xv.swapaxes(0, 1))
        ).swapaxes(0, 1)

        keys = self.repeat_kv(keys, self.n_rep)
        values = self.repeat_kv(values, self.n_rep)

        xq = xq.swapaxes(1, 2)
        keys = keys.swapaxes(1, 2)
        values = values.swapaxes(1, 2)

        var scores = (xq @ keys.swapaxes(2, 3)) * ops.rsqrt(head_dim)
        scores = scores + self.attention_mask(start_pos, seq_len)
        var output = ops.softmax(scores) @ values
        output = output.swapaxes(1, 2).reshape(batch_size, seq_len, -1)
        return output @ self.wo, xk, xv


@value
struct FeedForward:
    var w1: Symbol
    var w2: Symbol
    var w3: Symbol

    def __call__(self, input: Symbol) -> Symbol:
        return (ops.silu(input @ self.w1) * (input @ self.w3)) @ self.w2


@value
struct RMSNorm:
    var eps: Float64
    var weight: Symbol

    def __call__(self, input: Symbol) -> Symbol:
        var scale = ops.rsqrt(ops.mean(input**2.0, axis=-1) + self.eps)
        # Since norm weights are float32, cast to input dtype to avoid
        # promoting the result to float32 when the input is float16.
        return input * scale * ops.cast(self.weight, input.tensor_type().dtype)


@value
struct TransformerBlock[float_dtype: DType = DType.float32](CollectionElement):
    var attention: Attention[float_dtype]
    var feed_forward: FeedForward
    var attention_norm: RMSNorm
    var ffn_norm: RMSNorm

    def __call__(
        self,
        input: Symbol,
        start_pos: Symbol,
        freqs_cis: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> Tuple[Symbol, Symbol, Symbol]:
        var attention_out: Symbol
        var k_cache_update: Symbol
        var v_cache_update: Symbol
        attention_out, k_cache_update, v_cache_update = self.attention(
            self.attention_norm(input), start_pos, freqs_cis, k_cache, v_cache
        )
        var h = input + attention_out
        h = h + self.feed_forward(self.ffn_norm(h))
        return h, k_cache_update, v_cache_update


@value
struct Embedding:
    var weights: Symbol

    def __call__(self, input: Symbol) -> Symbol:
        return ops.gather(self.weights, input, axis=0)


@value
struct Transformer[float_dtype: DType = DType.float32]:
    alias max_seq_len = 2048
    alias theta = 10000.0

    var dim: Int
    var n_heads: Int

    var embedding: Embedding
    var layers: List[TransformerBlock[float_dtype]]
    var norm: RMSNorm
    var output: Symbol

    def freqs_cis(self, start_pos: Symbol, seq_len: Symbol) -> Symbol:
        var g = start_pos.graph()
        var n = self.dim // self.n_heads
        var iota = g.range[DType.float32](0, n - 1, 2)
        var freqs = 1.0 / (Self.theta ** (iota / n))
        var t = g.range[float_dtype](0, Self.max_seq_len * 2.0, 1)
        freqs = t.reshape(-1, 1) * freqs.reshape(1, -1)

        var retval = ops.stack(
            List[Symbol](ops.cos(freqs), ops.sin(freqs)), axis=-1
        )
        return ops.cast(retval[start_pos : start_pos + seq_len], float_dtype)

    def __call__(
        self, tokens: Symbol, k_cache: Symbol, v_cache: Symbol
    ) -> Tuple[Symbol, Symbol, Symbol]:
        var start_pos = ops.shape_of(k_cache)[0]
        var h = self.embedding(tokens)
        var freqs_cis = self.freqs_cis(start_pos, ops.shape_of(tokens)[1])

        var k_cache_updates = List[Symbol]()
        var v_cache_updates = List[Symbol]()
        for i in range(len(self.layers)):
            var k_cache_layer_update: Symbol
            var v_cache_layer_update: Symbol
            h, k_cache_layer_update, v_cache_layer_update = self.layers[i](
                h, start_pos, freqs_cis, k_cache[i, axis=1], v_cache[i, axis=1]
            )
            k_cache_updates.append(k_cache_layer_update.swapaxes(0, 1))
            v_cache_updates.append(v_cache_layer_update.swapaxes(0, 1))

        return (
            self.norm(h) @ self.output,
            ops.stack(k_cache_updates, axis=1),
            ops.stack(v_cache_updates, axis=1),
        )


struct Llama2[ModelT: LoadableModel, float_dtype: DType = DType.float32]:
    alias batch_size = 1

    var params: ModelT
    var hyperparams: LlamaHParams
    var enable_custom_rope_kernel: Bool

    fn __init__(
        inout self, model_path: Path, enable_custom_rope_kernel: Bool = False
    ) raises:
        constrained[
            float_dtype.is_floating_point(), "expected float model dtype"
        ]()

        self.params = ModelT(model_path)
        self.hyperparams = self.params.hyperparams()
        self.enable_custom_rope_kernel = enable_custom_rope_kernel

    fn build_graph(inout self, name: String) raises -> Graph:
        var cache_type = TensorType(
            float_dtype,
            Dim.dynamic(),
            self.hyperparams.n_layers,
            Self.batch_size,
            self.hyperparams.n_kv_heads,
            self.hyperparams.head_dim,
        )
        var tokens_type = TensorType(
            DType.int64, self.batch_size, Dim.dynamic()
        )
        var g = Graph(name, List[Type](tokens_type, cache_type, cache_type))

        @parameter
        fn weight[
            dtype: DType = float_dtype
        ](name: String, i: Optional[Int] = None) raises -> Symbol:
            return g.constant(self.params.get[dtype](name, i))

        var layers = List[TransformerBlock[float_dtype]]()
        for i in range(self.hyperparams.n_layers):
            var layer = TransformerBlock(
                attention=Attention[float_dtype](
                    n_heads=self.hyperparams.n_heads,
                    n_kv_heads=self.hyperparams.n_kv_heads,
                    head_dim=self.hyperparams.head_dim,
                    dim=self.hyperparams.dims,
                    n_rep=self.hyperparams.n_rep,
                    enable_custom_rope_kernel=self.enable_custom_rope_kernel,
                    wq=weight("attn_q", i).swapaxes(-1, -2),
                    wk=weight("attn_k", i).swapaxes(-1, -2),
                    wv=weight("attn_v", i).swapaxes(-1, -2),
                    wo=weight("attn_output", i).swapaxes(-1, -2),
                ),
                feed_forward=FeedForward(
                    w1=weight("ffn_gate", i).swapaxes(-1, -2),
                    w2=weight("ffn_down", i).swapaxes(-1, -2),
                    w3=weight("ffn_up", i).swapaxes(-1, -2),
                ),
                attention_norm=RMSNorm(
                    # Use float32 norm weights rather than float_dtype: they
                    # are vectors and small, so GGUF stores these as float32.
                    self.hyperparams.norm_eps,
                    weight[DType.float32]("attn_norm", i),
                ),
                ffn_norm=RMSNorm(
                    self.hyperparams.norm_eps,
                    weight[DType.float32]("ffn_norm", i),
                ),
            )
            layers.append(layer)

        var logits: Symbol
        var k_cache = g[1]
        var v_cache = g[2]
        logits, k_cache, v_cache = Transformer[float_dtype](
            dim=self.hyperparams.dims,
            n_heads=self.hyperparams.n_heads,
            embedding=Embedding(weight("token_embd")),
            layers=layers,
            norm=RMSNorm(
                self.hyperparams.norm_eps, weight[DType.float32]("output_norm")
            ),
            output=weight("output").swapaxes(-1, -2),
        )(tokens=g[0], k_cache=k_cache, v_cache=v_cache)

        logits = ops.gather(logits, g.scalar[DType.int64](-1, rank=1), axis=1)
        var next_token = ops.arg_max(
            logits.reshape(-1, self.hyperparams.vocab_size), axis=-1
        )
        g.output(
            List[Symbol](
                next_token.reshape(self.batch_size, -1), k_cache, v_cache
            )
        )
        return g
