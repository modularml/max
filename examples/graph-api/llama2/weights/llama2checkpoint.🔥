# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

from memory import memcpy
from tensor import Tensor, TensorShape
from collections import Optional
from pathlib import Path

from .loadable_model import LlamaHParams, LoadableModel


@value
@register_passable("trivial")
struct Config:
    var dim: Int  # transformer dimension
    var hidden_dim: Int  # for ffn layers
    var n_layers: Int  # number of layers
    var n_heads: Int  # number of query heads
    var n_kv_heads: Int  # number of key/value heads (can be < query heads because of multiquery)
    var vocab_size: Int  # vocabulary size, usually 256 (byte-level)
    var seq_len: Int  # max sequence length
    var head_size: Int  # dimension of attention head activations


@value
struct TensorRef[type: DType]:
    # Not owned by this `TensorRef`.
    var offset: DTypePointer[type]
    var shape: TensorShape

    @always_inline
    fn nelems(self) -> Int:
        # Number of elements in a single tensor.
        return self.shape.num_elements()


@value
struct LlamaCFile(LoadableModel):
    """Checkpoint file from karpathy/llama2.c.

    This is a simple format that groups each "type" of weight in one large
    contiguous block.
    So for example wq layer 0 weights start at some offset N, then wq layer 1
    weights immediately follow, and so on.
    See: https://github.com/karpathy/llama2.c/blob/d9862069e7ef665fe6309e3c17398ded2f121bf5/run.c#L111.
    """

    var config: Config
    var weights_ptr: DTypePointer[DType.int8]

    fn __init__(inout self, model_path: Path) raises:
        with open(model_path, "r") as f:

            @parameter
            @always_inline
            fn read_int32() raises -> Int32:
                var bytes_tensor = Tensor[DType.int8](
                    f.read_bytes(sizeof[Int32]())
                )
                var result = bytes_tensor.data().bitcast[DType.int32]().load()
                _ = bytes_tensor ^
                return result

            var dim = read_int32()
            var hidden_dim = read_int32()
            var n_layers = read_int32()
            var n_heads = read_int32()
            var n_kv_heads = read_int32()
            var vocab_size = read_int32()
            if vocab_size < 0:
                raise "negative vocab size unsupported"
            var seq_len = read_int32()

            self.config = Config(
                int(dim),
                int(hidden_dim),
                int(n_layers),
                int(n_heads),
                int(n_kv_heads),
                int(vocab_size),
                int(seq_len),
                head_size=int(dim // n_heads),
            )

            var bytes_tensor = Tensor[DType.int8](f.read_bytes())
            self.weights_ptr = bytes_tensor._steal_ptr()

    fn __moveinit__(inout self, owned existing: Self):
        self.config = existing.config
        self.weights_ptr = existing.weights_ptr

    fn __del__(owned self):
        self.weights_ptr.free()

    fn token_embd[type: DType](self) -> TensorRef[type]:
        return TensorRef[type](
            self.weights_ptr.bitcast[type](),
            TensorShape(self.config.vocab_size, self.config.dim),
        )

    fn rms_att[type: DType](self) -> TensorRef[type]:
        return TensorRef(
            self.token_embd[type]().offset + self.token_embd[type]().nelems(),
            TensorShape(self.config.dim),
        )

    fn wq[type: DType](self) -> TensorRef[type]:
        return TensorRef(
            self.rms_att[type]().offset
            + (self.config.n_layers * self.rms_att[type]().nelems()),
            TensorShape(
                self.config.n_heads * self.config.head_size, self.config.dim
            ),
        )

    fn wk[type: DType](self) -> TensorRef[type]:
        return TensorRef(
            self.wq[type]().offset
            + (self.config.n_layers * self.wq[type]().nelems()),
            TensorShape(
                self.config.n_kv_heads * self.config.head_size, self.config.dim
            ),
        )

    fn wv[type: DType](self) -> TensorRef[type]:
        return TensorRef(
            self.wk[type]().offset
            + (self.config.n_layers * self.wk[type]().nelems()),
            TensorShape(
                self.config.n_kv_heads * self.config.head_size, self.config.dim
            ),
        )

    fn wo[type: DType](self) -> TensorRef[type]:
        return TensorRef(
            self.wv[type]().offset
            + (self.config.n_layers * self.wv[type]().nelems()),
            TensorShape(
                self.config.dim, self.config.n_heads * self.config.head_size
            ),
        )

    fn rms_ffn[type: DType](self) -> TensorRef[type]:
        return TensorRef(
            self.wo[type]().offset
            + (self.config.n_layers * self.wo[type]().nelems()),
            TensorShape(self.config.dim),
        )

    fn w1[type: DType](self) -> TensorRef[type]:
        return TensorRef(
            self.rms_ffn[type]().offset
            + (self.config.n_layers * self.rms_ffn[type]().nelems()),
            TensorShape(self.config.hidden_dim, self.config.dim),
        )

    fn w2[type: DType](self) -> TensorRef[type]:
        return TensorRef(
            self.w1[type]().offset
            + (self.config.n_layers * self.w1[type]().nelems()),
            TensorShape(self.config.dim, self.config.hidden_dim),
        )

    fn w3[type: DType](self) -> TensorRef[type]:
        return TensorRef(
            self.w2[type]().offset
            + (self.config.n_layers * self.w2[type]().nelems()),
            TensorShape(self.config.hidden_dim, self.config.dim),
        )

    fn rms_final[type: DType](self) -> TensorRef[type]:
        return TensorRef(
            self.w3[type]().offset
            + (self.config.n_layers * self.w3[type]().nelems()),
            TensorShape(self.config.dim),
        )

    fn wcls[type: DType](self) -> TensorRef[type]:
        return TensorRef(
            self.rms_final[type]().offset + self.rms_final[type]().nelems()
            # Skip what used to be freq_cis_{real,img}
            + int(2 * (self.config.seq_len * self.config.head_size // 2)),
            TensorShape(self.config.vocab_size, self.config.dim),
        )

    fn get[
        type: DType
    ](
        inout self, key: String, layer_idx: Optional[Int] = None
    ) raises -> Tensor[type]:
        # Heap allocates and copies output, which is owned by the caller.
        var tensor_ref: TensorRef[type]
        if key == "token_embd":
            tensor_ref = self.token_embd[type]()
        elif key == "output_norm":
            tensor_ref = self.rms_final[type]()
        elif key == "output":
            if self.config.vocab_size > 0:
                tensor_ref = self.token_embd[type]()
            else:
                tensor_ref = self.wcls[type]()
        elif key == "attn_q":
            tensor_ref = self.wq[type]()
        elif key == "attn_k":
            tensor_ref = self.wk[type]()
        elif key == "attn_v":
            tensor_ref = self.wv[type]()
        elif key == "attn_output":
            tensor_ref = self.wo[type]()
        elif key == "ffn_gate":
            tensor_ref = self.w1[type]()
        elif key == "ffn_down":
            tensor_ref = self.w2[type]()
        elif key == "ffn_up":
            tensor_ref = self.w3[type]()
        elif key == "attn_norm":
            tensor_ref = self.rms_att[type]()
        elif key == "ffn_norm":
            tensor_ref = self.rms_ffn[type]()
        else:
            raise "key not found"

        var ptr = DTypePointer[type].alloc(tensor_ref.nelems())
        var layer_offset = layer_idx.value() * tensor_ref.nelems() if layer_idx else 0
        memcpy(ptr, tensor_ref.offset + layer_offset, tensor_ref.nelems())

        return Tensor(ptr, tensor_ref.shape)

    fn hyperparams(self) -> LlamaHParams:
        return LlamaHParams(
            dims=self.config.dim,
            n_layers=self.config.n_layers,
            n_heads=self.config.n_heads,
            norm_eps=1e-5,
            n_kv_heads=self.config.n_kv_heads,
            vocab_size=self.config.vocab_size,
            head_dim=self.config.dim // self.config.n_heads,
            n_rep=self.config.n_heads // self.config.n_kv_heads,
        )
