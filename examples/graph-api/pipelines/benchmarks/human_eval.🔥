# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Wrapper for HumanEval module.

HumanEval is a set of evaluation tasks for code generating models.
Original paper: https://arxiv.org/abs/2107.03374

Instructions for running this benchmark on your model:

1. Install HumanEval:
    git clone https://github.com/openai/human-eval
    pip install -e human-eval
2. Import and use HumanEval in your model:
    ```
        from human_eval import HumanEval
        human_eval = HumanEval()
        problems = eval_benchmark.get_problems()
        for task_id in problems:
            for _ in range(SAMPLES_PER_TASK):
                # Define `generate` to return the completed code.
                completion = generate(str(problems[task_id]))
                eval_benchmark.add_sample(task_id, completion)

    ```
3. Run `evaluate_functional_correctness samples.jsonl`.
   Make sure to uncomment the `exec` line in `human-eval/execution.py`, which
   is commented by default so let users know that this script runs untrusted
   machine-generated code.
"""

from pathlib import Path
from python import Python, PythonObject


struct HumanEval:
    var _human_eval_module: PythonObject
    var _generated_samples: PythonObject
    var samples_per_task: Int

    def __init__(out self, samples_per_task: Int = 1):
        try:
            self._human_eval_module = Python.import_module("human_eval.data")
        except e:
            raise (
                "Unable to import HumanEval module, make sure to install it:"
                + "\n\tgit clone https://github.com/openai/human-eval"
                + "\n\tpip install -e human-eval"
            )
        self._generated_samples = Python.list()
        self.samples_per_task = samples_per_task

    def get_problems(self) -> PythonObject:
        return self._human_eval_module.read_problems()

    def add_sample(mut self, task_id: PythonObject, completion: String):
        sample = Python.dict()
        sample["task_id"] = task_id
        sample["completion"] = PythonObject(completion)
        self._generated_samples.append(sample)

    def write[PathLike: PathLike](self, file_path: PathLike):
        self._human_eval_module.write_jsonl(
            file_path.__fspath__(), self._generated_samples
        )
