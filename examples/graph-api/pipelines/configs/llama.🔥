# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

from collections import Dict
from pathlib import Path
from .common import check_url_exists
from .parse_args import OptionType, OptionTypeEnum, OptionValue
from .registry import ConfigRegistry, ConfigRegistryDict
from max.graph.quantization import (
    Float32Encoding,
    QuantizationEncoding,
    Q4_0Encoding,
    Q4_KEncoding,
    Q6_KEncoding,
    BFloat16Encoding,
)


def get_llama_base_default_config() -> Dict[String, OptionValue]:
    default_config = Dict[String, OptionValue]()
    default_config["batch-size"] = 1
    default_config["max-length"] = 512
    default_config["max-new-tokens"] = -1
    default_config["model-path"] = Path("")
    default_config["custom-ops-path"] = List[Path]()
    default_config["tokenizer-path"] = Path("")
    default_config["prompt"] = String("I believe the meaning of life is")
    default_config["quantization-encoding"] = String("q4_k")
    default_config["experimental-use-gpu"] = False
    default_config["temperature"] = 0.5
    default_config["min-p"] = 0.05
    default_config["warmup-pipeline"] = False
    default_config["mef-use-or-gen-path"] = String("")
    default_config["pad-to-multiple-of"] = 1
    return default_config


# fmt: off
def get_llama2_model_url(encoding: String) -> String:
    urls = Dict[String, String]()
    urls[Q4_0Encoding.id()] = "https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_0.gguf"
    urls[Q4_KEncoding.id()] = "https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf"
    urls[Q6_KEncoding.id()] = "https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q6_K.gguf"
    urls[Float32Encoding.id()] = "https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin"
    check_url_exists(urls, encoding)
    return urls[encoding]


def get_llama3_model_url(encoding: String) -> String:
    urls = Dict[String, String]()
    urls[Q4_0Encoding.id()] = "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_0.gguf"
    urls[Q4_KEncoding.id()] = "https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"
    urls[Q6_KEncoding.id()] = "https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q6_K.gguf"
    urls[BFloat16Encoding.id()] = "https://huggingface.co/ddh0/Meta-Llama-3-8B-Instruct-bf16-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-bf16.gguf"
    urls[Float32Encoding.id()] = "https://huggingface.co/brendanduke/Llama-3-8B-f32.gguf/resolve/main/llama3-8b-f32.gguf"
    check_url_exists(urls, encoding)
    return urls[encoding]

def get_llama3_1_model_url(encoding: String) -> String:
    urls = Dict[String, String]()
    urls[Q4_0Encoding.id()] = "https://huggingface.co/kaetemi/Meta-Llama-3.1-8B-Q4_0-GGUF/resolve/main/meta-llama-3.1-8b-q4_0.gguf"
    urls[Q4_KEncoding.id()] = "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
    urls[Q6_KEncoding.id()] = "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf"
    urls[BFloat16Encoding.id()] = "https://huggingface.co/bullerwins/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-bf16.gguf"
    urls[Float32Encoding.id()] = "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-f32.gguf"
    check_url_exists(urls, encoding)
    return urls[encoding]
# fmt: on


@value
struct LlamaConfigRegistry(ConfigRegistry):
    """
    This struct holds a dictionary of llama configs and their corresponding types
    as used by both llama2 and llama3.
    """

    var registry: ConfigRegistryDict

    def __init__(
        inout self,
        additional_pipeline_args: ConfigRegistryDict = ConfigRegistryDict(),
    ):
        """
        This constructor instantiates llama2 / llama3 config keys and their
        corresponding types. An optional pipeline args dict is also available
        for use if a specific pipeline has additional arguments.
        """
        self.registry = ConfigRegistryDict()
        self.registry["batch-size"] = OptionTypeEnum.INT
        self.registry["max-length"] = OptionTypeEnum.INT
        self.registry["max-new-tokens"] = OptionTypeEnum.INT
        self.registry["model-path"] = OptionTypeEnum.PATH
        self.registry["custom-ops-path"] = OptionTypeEnum.PATH_LIST
        self.registry["tokenizer-path"] = OptionTypeEnum.PATH
        self.registry["prompt"] = OptionTypeEnum.STRING
        self.registry["quantization-encoding"] = OptionTypeEnum.STRING
        self.registry["temperature"] = OptionTypeEnum.FLOAT
        self.registry["min-p"] = OptionTypeEnum.FLOAT
        self.registry["experimental-use-gpu"] = OptionTypeEnum.BOOL
        self.registry["warmup-pipeline"] = OptionTypeEnum.BOOL
        self.registry["mef-use-or-gen-path"] = OptionTypeEnum.STRING
        self.registry["pad-to-multiple-of"] = OptionTypeEnum.INT

        self.registry.update(additional_pipeline_args)

    def register(self, key: String, option_type: OptionType):
        self.registry[key] = option_type
