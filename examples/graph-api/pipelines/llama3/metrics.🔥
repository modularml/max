# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Common benchmarks and metrics for the Llama models."""

from collections import List, Optional
from time import now
from runtime.tracing import Trace, TraceLevel


struct Metrics:
    """A group of timings and throughput measurements for text generation."""

    var start_startup: Optional[Int]
    var end_startup: Optional[Int]
    var start_time_before_prompt: Optional[Int]
    var start_time_before_warmup: Optional[Int]
    var end_warmup: Optional[Int]
    var start_time_before_generation: Optional[Int]
    var start_time_before_context: Optional[Int]
    var end_time: Optional[Int]
    var tokens_in_prompt: Optional[Int]
    var tokens_generated: Int
    var traces: List[Trace[TraceLevel.OP]]

    def __init__(inout self):
        self.tokens_in_prompt = None
        self.start_startup = None
        self.end_startup = None
        self.start_time_before_prompt = None
        self.start_time_before_warmup = None
        self.end_warmup = None
        self.start_time_before_generation = None
        self.start_time_before_context = None
        self.end_time = None
        self.tokens_generated = 0
        self.traces = List[Trace[TraceLevel.OP]]()
        self.traces.append(Trace[TraceLevel.OP]("PipelineMetric"))
        self.traces[0].start()

    def set_tokens_in_prompt(inout self, tokens_in_prompt: Int):
        """Provides the count of tokens processed in the prompt."""
        self.tokens_in_prompt = tokens_in_prompt

    def begin_timing_startup(inout self):
        """Begins measurement of the pipeline startup time."""
        self.start_startup = now()
        self.traces.append(
            Trace[TraceLevel.OP]("startup", parent_id=self.traces[0].event_id)
        )
        self.traces[-1].start()

    def end_timing_startup(inout self):
        """Ends measurement of the pipeline startup time."""
        self.end_startup = now()
        self.traces[-1].end()
        _ = self.traces.pop()

    def begin_timing_prompt(inout self):
        """Begins timing from before prompt processing."""
        self.start_time_before_prompt = now()

    def begin_timing_warmup(inout self):
        """Begins timing from before an optional warmup run."""
        self.start_time_before_warmup = now()

    def end_timing_warmup(inout self):
        """Ends measurement of an optional warmup run."""
        self.end_warmup = now()

    def begin_timing_generation(inout self):
        """Begins timing from the first generated token."""
        self.start_time_before_generation = now()
        self.traces.append(
            Trace[TraceLevel.OP](
                "generate",
                detail="token:" + str(self.tokens_generated),
                parent_id=self.traces[0].event_id,
            )
        )
        self.traces[-1].start()

    def new_token(inout self):
        """Increments the total tokens generated and corresponding metrics."""
        if not self.start_time_before_context:
            # If this is the first token, store the current time for reporting
            # the time-to-first-token.
            self.start_time_before_context = now()
        else:
            # The first token is not included in the total tokens generated
            # (for computing eval throughput), because the rest of the tokens
            # will use the cached keys and values and thus will be generated much
            # faster.
            self.tokens_generated += 1
        self.traces[-1].end()
        _ = self.traces.pop()
        self.traces.append(
            Trace[TraceLevel.OP](
                "generate",
                detail="token:" + str(self.tokens_generated),
                parent_id=self.traces[0].event_id,
            )
        )
        self.traces[-1].start()

    def end_timing(inout self):
        """Ends timing token generation."""
        self.end_time = now()
        for trace in self.traces:
            trace[].end()

    def print(self):
        """Prints the final gathered metrics to the console."""
        if not self.start_time_before_context:
            # Text generation was never started, so no metrics to print.
            return
        start_context = self.start_time_before_context.value()
        tokens_in_prompt = self.tokens_in_prompt.value()
        if not self.start_time_before_prompt:
            raise "timing was never started before the prompt, make sure to call `begin_timing_prompt()`"
        start_inc_prompt = self.start_time_before_prompt.value()
        if not self.start_time_before_generation:
            raise "timing was never started before text generation, make sure to call `begin_timing_generation()`"
        start = self.start_time_before_generation.value()
        if not self.end_time:
            raise "timing was never stopped, make sure to call `end_timing()`"
        end = self.end_time.value()
        print("Prompt size:", tokens_in_prompt)
        print("Output size:", self.tokens_generated)
        if self.start_startup and self.end_startup:
            print(
                "Startup time:",
                (self.end_startup.value() - self.start_startup.value()) * 1e-6,
                "ms",
            )
        if self.start_time_before_warmup and self.end_warmup:
            print(
                "Warmup time:",
                (
                    self.end_warmup.value()
                    - self.start_time_before_warmup.value()
                )
                * 1e-6,
                "ms",
            )
            print(
                "Time to first token:",
                (
                    (start_context - self.end_warmup.value())
                    + (self.start_time_before_warmup.value() - start_inc_prompt)
                )
                * 1e-6,
                "ms",
            )
        else:
            print(
                "Time to first token:",
                (start_context - start_inc_prompt) * 1e-6,
                "ms",
            )
        print(
            "Prompt eval throughput (context-encoding):",
            tokens_in_prompt / (start_context - start) * 1e9,
            "tokens per second",
        )
        print(
            "Eval throughput (token-generation):",
            self.tokens_generated / (end - start_context) * 1e9,
            "tokens per second",
        )
