# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""The body of the Llama 3 model definition."""

from collections import List, Optional
from pathlib import Path

from max.graph import (
    ops,
    Dim,
    Graph,
    TensorType,
    Type,
    Symbol,
    _OpaqueType as OpaqueType,
)
from max.graph.quantization import (
    Float32Encoding,
    BFloat16Encoding,
    Q4_0Encoding,
    Q4_KEncoding,
    Q6_KEncoding,
    QuantizationEncoding,
)
from pipelines.nn import (
    Attention,
    Embedding,
    RMSNorm,
    FeedForward,
    Transformer,
    TransformerBlock,
    KVCacheOptimizedTransformerBlock,
    KVCacheOptimizedTransformer,
    KVCacheOptimizedAttention,
)
from kv_cache.types import KVCacheStaticParams, ContiguousKVCacheCollection
from pipelines.weights.gguf import GGMLType, GGUFFile
from pipelines.weights.loadable_model import LlamaHParams


def encoding_id_from_ggml_type(ggml_type: GGMLType) -> String:
    """Returns the quantization encoding id for a given GGML tensor type."""
    if ggml_type is GGMLType.GGML_TYPE_F32:
        return Float32Encoding.id()
    elif ggml_type is GGMLType.GGML_TYPE_BF16:
        return BFloat16Encoding.id()
    elif ggml_type is GGMLType.GGML_TYPE_Q4_0:
        return Q4_0Encoding.id()
    elif ggml_type is GGMLType.GGML_TYPE_Q4_K:
        return Q4_KEncoding.id()
    elif ggml_type is GGMLType.GGML_TYPE_Q6_K:
        return Q6_KEncoding.id()

    raise "unknown GGML type:" + str(ggml_type)


struct Llama3_NaiveKVCache[encoding: QuantizationEncoding = Float32Encoding]:
    alias batch_size = 1

    var model: GGUFFile

    def __init__(out self, model_path: Path):
        self.model = GGUFFile(model_path)

    def build_graph(mut self, name: String) -> Graph:
        params = self.hyperparams()
        alias model_dtype = DType.bfloat16 if encoding.id() == BFloat16Encoding.id() else DType.float32
        cache_type = TensorType(
            DType.float32,
            "prev_seq_len",
            params.n_layers,
            Self.batch_size,
            params.n_kv_heads,
            params.head_dim,
        )
        tokens_type = TensorType(DType.int64, self.batch_size, "seq_len")
        attn_mask_type = TensorType(DType.bool, self.batch_size, "full_seq_len")
        g = Graph(
            name,
            List[Type](tokens_type, attn_mask_type, cache_type, cache_type),
        )

        @parameter
        def weight[
            weight_type: QuantizationEncoding = encoding
        ](
            name: String,
            layer: Optional[Int] = None,
            # Torch models store weights in a transposed format, and transpose again at use.
            # The q4_0 format transposes the weights compared with the fp32 formats.
            transpose: Bool = (
                weight_type.id() == Float32Encoding.id()
                or weight_type.id() == BFloat16Encoding.id()
            ),
        ) -> (Symbol, String):
            """Stages a constant op according to the quantization encoding."""

            # Set uint8 dtype if quantized and float32 / bfloat16 otherwise,
            # since all quantization encodings currently have uint8 dtype.
            alias dtype = DType.float32 if weight_type.id() == Float32Encoding.id() else (
                DType.bfloat16 if weight_type.id()
                == BFloat16Encoding.id() else DType.uint8
            )

            # Also return the quantization encoding for this constant.
            encoding_id = encoding_id_from_ggml_type(
                self.model.ggml_type(name, layer)
            )

            weight = g.constant(self.model.get[dtype](name, layer))
            if transpose:
                weight = weight.swapaxes(-1, -2)

            return weight, encoding_id

        def norm(name: String, layer: Optional[Int] = None) -> RMSNorm:
            # GGUF always stores these as float32.
            w, _ = weight[Float32Encoding](name, layer, transpose=False)
            return RMSNorm(params.norm_eps, w)

        # TODO: Remove the following check once KERN-627 is implemented.
        @parameter
        if encoding.id() == BFloat16Encoding.id():
            print(
                "Note: split KV cache flash attention currently unavailable for"
                " bfloat16, falling back to naive attention"
            )
            use_custom_attention = False
        else:
            use_custom_attention = True

        layers = List[TransformerBlock[model_dtype]]()
        for layer in range(params.n_layers):
            attention = Attention[model_dtype](
                n_heads=params.n_heads,
                n_kv_heads=params.n_kv_heads,
                head_dim=params.head_dim,
                dim=params.dims,
                use_custom_attention=use_custom_attention,
                wq=weight("attn_q", layer),
                wk=weight("attn_k", layer),
                wv=weight("attn_v", layer),
                wo=weight("attn_output", layer),
            )
            var feed_forward = FeedForward(
                w1=weight("ffn_gate", layer),
                w2=weight("ffn_down", layer),
                w3=weight("ffn_up", layer),
            )

            layers.append(
                TransformerBlock(
                    attention=attention,
                    feed_forward=feed_forward,
                    attention_norm=norm("attn_norm", layer),
                    ffn_norm=norm("ffn_norm", layer),
                )
            )

        # Llama 3.1 introduces a RoPE scaling factor, which will be present in
        # the GGUF weights for that model.
        try:
            rope_scaling = Optional[Symbol](
                weight[Float32Encoding]("rope_freqs", transpose=False)[0]
            )
        except:
            rope_scaling = None

        embedding = Embedding(weight("token_embd", transpose=False))
        model = Transformer(
            dim=params.dims,
            n_heads=params.n_heads,
            embedding=embedding,
            layers=layers,
            norm=norm("output_norm"),
            output=weight("output"),
            theta=500000.0,
            rope_scaling=rope_scaling,
        )

        # outputs (logits, k_cache, v_cache)
        # The KV cache is currently stored locally as float32, so cast between
        # inputs and outputs to convert model <-> cache datatypes.
        outputs = model(
            tokens=g[0],
            attention_mask=g[1],
            k_cache=ops.cast(g[2], model_dtype),
            v_cache=ops.cast(g[3], model_dtype),
        )
        logits = outputs[0]
        g.output(
            List(
                ops.cast(logits[-1, axis=1], DType.float32),
                ops.cast(outputs[1], DType.float32),
                ops.cast(outputs[2], DType.float32),
            )
        )
        return g

    def hyperparams(self) -> LlamaHParams:
        """Returns hyperparameters corresponding to this Llama 3 model."""
        return self.model.hyperparams()


struct Llama3[
    type: DType,
    kv_params: KVCacheStaticParams,
    encoding: QuantizationEncoding = Float32Encoding,
]:
    alias batch_size = 1

    var model: GGUFFile

    def __init__(out self, model_path: Path):
        self.model = GGUFFile(model_path)

    def build_graph(mut self, name: String) -> Graph:
        params = self.hyperparams()
        cache_type = OpaqueType(
            ContiguousKVCacheCollection[type, kv_params].id()
        )
        tokens_type = TensorType(DType.int64, self.batch_size, "seq_len")
        mask_type = TensorType(DType.bool, self.batch_size, "full_seq_len")
        g = Graph(
            name,
            List[Type](tokens_type, mask_type, cache_type),
        )

        @parameter
        def weight[
            type_: DType = type, weight_type: QuantizationEncoding = encoding
        ](
            name: String,
            layer: Optional[Int] = None,
            # Torch models store weights in a transposed format, and transpose again at use.
            # The q4_0 format transposes the weights compared with the fp32 formats.
            transpose: Bool = (
                weight_type.id() == Float32Encoding.id()
                or weight_type.id() == BFloat16Encoding.id()
            ),
        ) -> (Symbol, String):
            """Stages a constant op according to the quantization encoding."""

            # Set uint8 dtype if quantized and float32 / bfloat16 otherwise,
            # since all quantization encodings currently have uint8 dtype.

            # Also return the quantization encoding for this constant.
            encoding_id = encoding_id_from_ggml_type(
                self.model.ggml_type(name, layer)
            )

            weight = g.constant(self.model.get[type_](name, layer))
            if transpose:
                weight = weight.swapaxes(-1, -2)

            return weight, encoding_id

        def norm(name: String, layer: Optional[Int] = None) -> RMSNorm:
            # GGUF always stores these as float32.
            w, _ = weight[DType.float32, Float32Encoding](
                name, layer, transpose=False
            )
            return RMSNorm(params.norm_eps, w)

        layers = List[KVCacheOptimizedTransformerBlock[type, kv_params]]()
        for layer in range(params.n_layers):
            wq = weight("attn_q", layer)[0]
            wk = weight("attn_k", layer)[0]
            wv = weight("attn_v", layer)[0]
            wqkv = ops.concat(List[Symbol](wq, wk, wv), axis=1)
            attention = KVCacheOptimizedAttention[type, kv_params](
                n_heads=params.n_heads,
                dim=params.dims,
                wqkv=wqkv.swapaxes(0, 1),
                wo=weight("attn_output", layer),
                layer_idx=g.scalar(UInt32(layer)),
            )

            var feed_forward = FeedForward(
                w1=weight("ffn_gate", layer),
                w2=weight("ffn_down", layer),
                w3=weight("ffn_up", layer),
            )

            layers.append(
                KVCacheOptimizedTransformerBlock[type, kv_params](
                    attention=attention,
                    feed_forward=feed_forward,
                    attention_norm=norm("attn_norm", layer),
                    ffn_norm=norm("ffn_norm", layer),
                )
            )

        # Llama 3.1 introduces a RoPE scaling factor, which will be present in
        # the GGUF weights for that model.
        try:
            rope_scaling = Optional[Symbol](
                weight[DType.float32, weight_type=Float32Encoding](
                    "rope_freqs", transpose=False
                )[0]
            )
        except:
            rope_scaling = None

        embedding = Embedding(weight("token_embd", transpose=False))
        model = KVCacheOptimizedTransformer[type, kv_params](
            dim=params.dims,
            n_heads=params.n_heads,
            embedding=embedding,
            layers=layers,
            norm=norm("output_norm"),
            output=weight("output"),
            # Llama3 changes this value from 10000 in Llama2
            theta=500000.0,
            rope_scaling=rope_scaling,
        )
        outputs = model(tokens=g[0], mask=g[1], kv_collection=g[2])
        logits = outputs[0]
        g.output(List[Symbol](logits[-1, axis=1], outputs[1]))
        return g

    def hyperparams(self) -> LlamaHParams:
        """Returns hyperparameters corresponding to this Llama 3 model."""
        return self.model.hyperparams()
