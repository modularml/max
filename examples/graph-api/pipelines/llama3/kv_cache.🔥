# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""KV cache for the Transformer."""

from buffer import Buffer
from max.driver import AnyMemory, Device, DeviceTensor
from max.tensor import TensorSpec, TensorShape
from memory import memcpy
from max.tensor import Tensor, TensorSpec, TensorShape

from max.engine import EngineTensorView

from ..weights.loadable_model import LlamaHParams


@value
struct KVCache:
    """View into the KV cache backing `Tensor`."""

    var keys: DeviceTensor
    var values: DeviceTensor
    var sequence_length: Int

    def __init__(
        mut self,
        hp: LlamaHParams,
        max_length: Int,
        batch_size: Int,
        cpu_device: Device,
    ):
        spec = TensorSpec(
            DType.float32,
            max_length,
            hp.n_layers,
            batch_size,
            hp.n_kv_heads,
            hp.head_dim,
        )
        self.keys = cpu_device.allocate(spec, name=String("keys"))
        self.values = cpu_device.allocate(spec, name=String("values"))

        self.sequence_length = 0

    def update(mut self, owned keys: AnyMemory, owned values: AnyMemory):
        """Updates the KV Cache with data from new tokens."""
        cpu_device = self.keys.device()
        keys_tensor = keys^.to_device_tensor().move_to(cpu_device)
        values_tensor = values^.to_device_tensor().move_to(cpu_device)

        seqlen = keys_tensor.spec.bytecount() // self._offset(1)
        # This is doing the equivalent of
        #   self.keys[self.sequence_length:self.sequence_length + seqlen, ...] = keys
        #   self.values[self.sequence_length:self.sequence_length + seqlen, ...] = values
        start_pos = self._offset(self.sequence_length)
        update_size = self._offset(seqlen)
        memcpy(
            self.keys.unsafe_ptr() + start_pos,
            keys_tensor.unsafe_ptr(),
            update_size,
        )
        memcpy(
            self.values.unsafe_ptr() + start_pos,
            values_tensor.unsafe_ptr(),
            update_size,
        )
        self.sequence_length += seqlen

    def keys_view(self, device: Device) -> DeviceTensor:
        """Copies the keys tensor from the cache for use in the model."""
        cpu_device = self.keys.device()
        keys_copy = DeviceTensor(
            self._spec(self.sequence_length), cpu_device, String("keys_view")
        )
        memcpy(
            keys_copy.unsafe_ptr(),
            self.keys.unsafe_ptr(),
            self._offset(self.sequence_length),
        )
        return keys_copy^.move_to(device)

    def values_view(self, device: Device) -> DeviceTensor:
        """Copies the values tensor from the cache for use in the model."""
        cpu_device = self.values.device()
        values_copy = DeviceTensor(
            self._spec(self.sequence_length), cpu_device, String("values_view")
        )
        memcpy(
            values_copy.unsafe_ptr(),
            self.values.unsafe_ptr(),
            self._offset(self.sequence_length),
        )
        return values_copy^.move_to(device)

    def _spec(self, seq_len: Int) -> TensorSpec:
        """The shape of a key or value tensor of a given sequence length."""
        s = self.keys.spec
        return TensorSpec(DType.float32, seq_len, s[1], s[2], s[3], s[4])

    def _offset(self, seq_len: Int) -> Int:
        """The memory size, in bytes, for a key or value tensor of a given sequence length.
        """
        return self._spec(seq_len).num_elements() * 4
