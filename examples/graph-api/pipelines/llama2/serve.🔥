# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

import sys
from collections import List, Optional
from pathlib import cwd, Path
from python.python import _get_global_python_itf, Python, CPython
from runtime.llcl import Runtime, TaskGroup
from sys.ffi import DLHandle
from utils.index import Index
from time import sleep

from max.engine import InferenceSession, Model, TensorMap
from max.engine._utils import handle_from_config, call_dylib_func
from max.graph import Graph
from max.graph.quantization import (
    Float32Encoding,
    QuantizationEncoding,
    Q4_0Encoding,
)
from max.serve.http.runtime import PythonEntry
from max.serve.server import InferenceServer
from max.serve.service import (
    InferenceRequest,
    InferenceResponse,
    InferenceService,
)
from max.serve._serve_rt import (
    CServerAsync,
    InferenceRequestImpl,
    InferenceResponseImpl,
)

from tensor import Tensor, TensorShape, TensorSpec

from .kv_cache import KVCache
from .model.llama import Llama2
from .tokenizer.bpe import BPETokenizer
from ..samplers.weighted_sampler import WeightedSampler
from ..tokenizer import AutoTokenizer, Tokenizer
from ..weights.gguf import GGUFFile
from ..weights.llama2checkpoint import LlamaCFile
from ..weights.loadable_model import LlamaHParams, LoadableModel

from .run import (
    Config,
    compile_graph,
    execute,
    run_outer,
    _generate_text_with_tokenizer,
)


struct Llama2InferenceService[
    ModelT: LoadableModel,
    EncodingT: QuantizationEncoding = Float32Encoding,
    TokenizerT: Tokenizer = AutoTokenizer,
](InferenceService):
    """Inference service for Llama2."""

    var _config: Config
    var _tokenizer: TokenizerT
    var _session: InferenceSession

    var _model: Llama2[ModelT, EncodingT]
    var _graph: Graph
    var _compiled_model: Model

    var _lib: DLHandle
    var _server_ptr: DTypePointer[DType.invalid]
    var _json_module: PythonObject

    fn __init__(
        inout self,
        owned config: Config,
        owned server_ptr: DTypePointer[DType.invalid],  # TODO: Remove
        owned tokenizer: TokenizerT,
        owned session: InferenceSession,
    ) raises:
        self._config = config^
        self._server_ptr = server_ptr
        self._tokenizer = tokenizer^
        self._session = session^
        self._lib = handle_from_config("serving", ".serve_lib")
        self._json_module = Python.import_module("json")

        print("Building model...")
        self._model = Llama2[ModelT, EncodingT](
            self._config.model_path,
            enable_custom_rope_kernel=self._config.enable_custom_rope_kernel,
        )
        self._graph = self._model.build_graph("llama_model")
        self._compiled_model = self._session.load(
            self._graph, custom_ops_paths=self._config.custom_ops_paths
        )

    fn __del__(owned self):
        _ = self._config^
        _ = self._tokenizer^
        _ = self._session^
        _ = self._model^
        _ = self._graph^
        _ = self._compiled_model^

    fn init(self, inout server: InferenceServer) raises:
        server._impl.init(self._compiled_model)

    fn infer[
        req_type: InferenceRequest, resp_type: InferenceResponse
    ](inout self, request: req_type, inout response: resp_type) raises -> None:
        var respOr = Variant[resp_type, Error](response^)
        var rt = Runtime()
        rt.run(self.async_infer(request, respOr))
        if respOr.isa[Error]():
            raise respOr.unsafe_take[Error]()
        else:
            response = respOr.unsafe_take[resp_type]()

    fn handle_openai[
        handle_type: fn (PythonEntry) capturing raises -> None,
        req_type: InferenceRequest,
    ](self, request: req_type) raises:
        var api_type = request.get_api_type()
        var payload_type = request.get_payload_type()
        if api_type == 1:
            # OpenAI
            if payload_type == 0:
                # gRPC
                raise Error(
                    "OpenAI API compatibility is only supported via HTTP."
                )
            else:
                # HTTP
                var entry = PythonEntry()
                call_dylib_func[NoneType](
                    self._lib,
                    "M_OpenAIInferenceRequest_fillEntry",
                    self._server_ptr,
                    request.get_ptr(),
                    UnsafePointer.address_of(entry),
                )
                handle_type(entry)

    async fn async_infer[
        req_type: InferenceRequest, resp_type: InferenceResponse
    ](
        inout self, request: req_type, inout response: Variant[resp_type, Error]
    ) -> None:
        @parameter
        def handle(entry: PythonEntry) -> None:
            cpython = _get_global_python_itf().cpython()
            state = cpython.PyGILState_Ensure()

            body = PythonObject(entry.request)
            cpython.Py_IncRef(entry.request)
            stream = False
            if body.__contains__("stream") and body["stream"]:
                stream = True

            # Tokenize prompt and message contents.
            var raw_prompt: String = ""
            for node in body["messages"]:
                raw_prompt += (
                    str(node["role"]) + ":" + str(node["content"]) + "\n"
                )

            prompt = self._tokenizer.encode(raw_prompt, bos=String("\n<s>\n"))
            sampler = WeightedSampler(
                self._config.temperature, self._config.min_p
            )
            tokens = Tensor[DType.int64](TensorShape(1, len(prompt)))
            for i in range(len(prompt)):
                tokens[Index(0, i)] = prompt[i]

            outputs = List[String]()
            kv_cache = KVCache(
                self._model.model.hyperparams(),
                self._config.max_tokens,
                self._config.batch_size,
            )

            parent = PythonObject(entry.handler).parent
            cpython.Py_IncRef(entry.handler)

            resp = PythonObject(entry.response)
            cpython.Py_IncRef(entry.response)
            if stream:
                parent.send_response(200)
                parent.send_header("Content-type", "text/event-stream")
                parent.end_headers()

            # The first iteration caches the entire prompt and all subsequent
            # iterations generate one token.
            # Avoid overrunning the cache by setting the trip count accordingly.
            for _ in range(prompt.size, self._config.max_tokens + 1):
                logits = execute(
                    self._session, self._compiled_model, tokens, kv_cache
                )
                var token: SIMD[DType.int64, 1] = sampler.sample(
                    logits
                ).selected
                tokens = Tensor(TensorShape(1, 1), token)

                if self._tokenizer.is_end_of_text(
                    tokens[Index(0, tokens.shape()[1] - 1)]
                ):
                    break

                next_token = self._tokenizer.decode(token)
                if not stream:
                    outputs.append(next_token)
                else:
                    # Write chunk response if streaming.
                    var chunk = Python.dict()
                    var choices = Python.list()
                    var choice = Python.dict()
                    var delta = Python.dict()
                    delta["content"] = next_token
                    choice["index"] = 0
                    choice["delta"] = delta
                    choices.append(choice)
                    chunk["choices"] = choices

                    var json_str = self._json_module.dumps(chunk).encode(
                        encoding="utf_8"
                    )
                    try:
                        parent.wfile.write(json_str)
                        parent.wfile.flush()
                    except BrokenPipeError:
                        break

            # Write complete response if not streaming.
            if not stream:
                var raw_message: String = ""
                for output in outputs:
                    raw_message += output[]

                choice = Python.dict()
                message = Python.dict()
                message["role"] = "assistant"
                message["content"] = raw_message
                choice["index"] = 0
                choice["message"] = message

                choices = Python.list()
                choices.append(choice)
                resp["choices"] = choices

                parent.send_response(200)
                parent.send_header("Content-Type", "application/json")
                parent.end_headers()
                var json_str = self._json_module.dumps(resp).encode(
                    encoding="utf_8"
                )
                parent.wfile.write(json_str)

            # TODO: Add error handling.
            cpython.PyGILState_Release(state)

        try:
            # TODO: Fold into the ProtocolHandler eventually.
            self.handle_openai[handle, req_type](request)
        except e:
            response.set[Error](e)


def serve_inner[
    ModelT: LoadableModel,
    EncodingT: QuantizationEncoding,
    TokenizerT: Tokenizer = AutoTokenizer,
](owned config: Config, owned tokenizer: TokenizerT):
    session = InferenceSession()
    server = InferenceServer.create[True]("0.0.0.0:8000", session)
    service = Llama2InferenceService[ModelT, EncodingT, TokenizerT](
        config^, server._impl._impl._ptr, tokenizer^, session
    )
    print("Listening on port 8000!")
    service.init(server)
    server.serve(service)
    _ = server^
    _ = service^


def serve[
    ModelT: LoadableModel,
    EncodingT: QuantizationEncoding = Float32Encoding,
](config: Config):
    if AutoTokenizer.is_available():
        try:
            serve_inner[ModelT, EncodingT](
                config, AutoTokenizer("meta-llama/Llama-2-7b-hf")
            )
        except:
            print(
                "Unable to initialize AutoTokenizer, using Mojo tokenizer"
                " instead."
            )
            # Fall back to the Mojo tokenizer if setting up the AutoTokenizer
            # fails, for example due to lack of authentication.
            serve_inner[ModelT, EncodingT, BPETokenizer](
                config, BPETokenizer.from_file(config.tokenizer_path)
            )
    else:
        print(
            "Hugging Face `transformers` not installed, using Mojo tokenizer"
            " instead."
        )
        # Fall back to the Mojo tokenizer if `transformers` is not installed.
        serve_inner[ModelT, EncodingT, BPETokenizer](
            config, BPETokenizer.from_file(config.tokenizer_path)
        )


def llama2_serve():
    run_outer[serve]()
