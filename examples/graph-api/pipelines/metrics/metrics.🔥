# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Common metrics for pipeline benchmarks."""

from collections import List, Optional
from python import Python
from time import monotonic
from runtime.tracing import Trace, TraceLevel


struct Metrics:
    """A group of timings and throughput measurements for text generation."""

    var start_startup: Optional[UInt]
    var end_startup: Optional[UInt]
    var start_time_before_prompt: Optional[UInt]
    var start_time_before_warmup: Optional[UInt]
    var end_warmup: Optional[UInt]
    var start_time_before_tokenization: Optional[UInt]
    var end_tokenization: Optional[UInt]
    var start_time_before_generation: Optional[UInt]
    var start_time_before_context: Optional[UInt]
    var end_time: Optional[UInt]
    var tokens_in_prompt: Optional[Int]
    var tokens_generated: Int
    var traces: List[Trace[TraceLevel.OP]]

    def __init__(out self):
        self.tokens_in_prompt = None
        self.start_startup = None
        self.end_startup = None
        self.start_time_before_prompt = None
        self.start_time_before_warmup = None
        self.end_warmup = None
        self.start_time_before_tokenization = None
        self.end_tokenization = None
        self.start_time_before_generation = None
        self.start_time_before_context = None
        self.end_time = None
        self.tokens_generated = 0
        self.traces = List[Trace[TraceLevel.OP]]()
        self.traces.append(Trace[TraceLevel.OP]("PipelineMetric"))
        self.traces[0].start()

    def set_tokens_in_prompt(mut self, tokens_in_prompt: Int):
        """Provides the count of tokens processed in the prompt."""
        self.tokens_in_prompt = tokens_in_prompt

    def begin_timing_startup(mut self):
        """Begins measurement of the pipeline startup time."""
        self.start_startup = monotonic()
        self.traces.append(
            Trace[TraceLevel.OP]("startup", parent_id=self.traces[0].event_id)
        )
        self.traces[-1].start()

    def end_timing_startup(mut self):
        """Ends measurement of the pipeline startup time."""
        self.end_startup = monotonic()
        self.traces[-1].end()
        _ = self.traces.pop()

    def begin_timing_prompt(mut self):
        """Begins timing from before prompt processing."""
        self.start_time_before_prompt = monotonic()

    def begin_timing_warmup(mut self):
        """Begins timing from before an optional warmup run."""
        if not self.start_startup or self.end_startup:
            raise "Error: Warmup should be included within startup time"
        self.start_time_before_warmup = monotonic()

    def end_timing_warmup(mut self):
        """Ends measurement of an optional warmup run."""
        if not self.start_startup or self.end_startup:
            raise "Error: Warmup should be included within startup time"
        self.end_warmup = monotonic()

    def begin_timing_tokenization(mut self):
        """Begins timing from before tokenization."""
        if not self.start_time_before_prompt or self.start_time_before_context:
            raise "Error: Tokenization should be included within TTFT"
        self.start_time_before_tokenization = monotonic()

    def end_timing_tokenization(mut self):
        """Ends measurement of tokenization."""
        if not self.start_time_before_prompt or self.start_time_before_context:
            raise "Error: Tokenization should be included within TTFT"
        self.end_tokenization = monotonic()

    def begin_timing_generation(mut self):
        """Begins timing from the first generated token."""
        self.start_time_before_generation = monotonic()
        self.traces.append(
            Trace[TraceLevel.OP](
                "generate",
                detail="token:" + str(self.tokens_generated),
                parent_id=self.traces[0].event_id,
            )
        )
        self.traces[-1].start()

    def new_token(mut self):
        """Increments the total tokens generated and corresponding metrics."""
        if not self.start_time_before_context:
            # If this is the first token, store the current time for reporting
            # the time-to-first-token.
            self.start_time_before_context = monotonic()
        else:
            # The first token is not included in the total tokens generated
            # (for computing eval throughput), because the rest of the tokens
            # will use the cached keys and values and thus will be generated much
            # faster.
            self.tokens_generated += 1
        self.traces[-1].end()
        _ = self.traces.pop()
        self.traces.append(
            Trace[TraceLevel.OP](
                "generate",
                detail="token:" + str(self.tokens_generated),
                parent_id=self.traces[0].event_id,
            )
        )
        self.traces[-1].start()

    def end_timing(mut self):
        """Ends timing token generation."""
        self.end_time = monotonic()
        for trace in self.traces:
            trace[].end()

    @staticmethod
    def get_current_mem_high_watermark():
        """Return the maximum memory usage, taken as MaxRSS, in kilobytes.

        This takes the high water mark starting with the invocation of the script
        and ending when this function is called.

        It includes both the calling process and any of its threads, but not
        child processes (RUSAGE_BOTH isn't in resource, for some reason).

        """
        py_resource = Python.import_module("resource")
        py_platform = Python.import_module("platform")

        max_rss = int(py_resource.getrusage(py_resource.RUSAGE_SELF).ru_maxrss)
        if py_platform.system() == "Darwin":
            return max_rss / 1024  # macOS reports maxRSS in bytes
        else:
            return max_rss  # linux reports maxRSS in kilobytes

    def print(self):
        """Prints the final gathered metrics to the console."""
        if not self.start_time_before_context:
            # Text generation was never started, so no metrics to print.
            return
        start_context = self.start_time_before_context.value()
        tokens_in_prompt = self.tokens_in_prompt.value()
        if not self.start_time_before_prompt:
            raise "timing was never started before the prompt, make sure to call `begin_timing_prompt()`"
        start_inc_prompt = self.start_time_before_prompt.value()
        if not self.start_time_before_generation:
            raise "timing was never started before text generation, make sure to call `begin_timing_generation()`"
        start = self.start_time_before_generation.value()
        if not self.end_time:
            raise "timing was never stopped, make sure to call `end_timing()`"
        end = self.end_time.value()
        print("Prompt size:", tokens_in_prompt)
        print("Output size:", self.tokens_generated)
        if self.start_startup and self.end_startup:
            print(
                "Startup time:",
                (self.end_startup.value() - self.start_startup.value()) * 1e-6,
                "ms",
            )
        if self.start_time_before_warmup and self.end_warmup:
            print(
                "\tWarmup time:",
                (
                    self.end_warmup.value()
                    - self.start_time_before_warmup.value()
                )
                * 1e-6,
                "ms",
            )
        print(
            "Time to first token:",
            (start_context - start_inc_prompt) * 1e-6,
            "ms",
        )
        if self.start_time_before_tokenization:
            start_tokenize = self.start_time_before_tokenization.value()
            end_tokenize = self.end_tokenization.value()
            print(
                "\tTokenization Time:",
                (end_tokenize - start_tokenize) * 1e-6,
                "ms",
            )
        print(
            "Prompt eval throughput (context-encoding):",
            tokens_in_prompt / (start_context - start) * 1e9,
            "tokens per second",
        )
        print(
            "Eval throughput (token-generation):",
            self.tokens_generated / (end - start_context) * 1e9,
            "tokens per second",
        )
        print(
            "Maximum Memory Usage (kb):",
            Metrics.get_current_mem_high_watermark(),
        )
