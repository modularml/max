# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Pipeline for loading a quantized Llama model trained on TinyStories.

The code is almost identical to `quantize_tinystories.ðŸ”¥` except instead of
loading and quantizing weights from the karpathy/llama.c file, this loads the
already-quantized weights from the MAX checkpoint saved by the first run of the
pipeline.
"""

from collections import Optional
from pathlib import cwd, Path

from max.driver import cpu_device
from max.engine import InferenceSession
from max.graph import ops, Dim, Graph, Symbol, TensorType, Type
from max.graph.checkpoint import load, TensorDict
from max.graph.quantization import (
    Float32Encoding,
    Q4_0Encoding,
    QuantizationEncoding,
)

from pipelines.llama2.tokenizer.bpe import BPETokenizer
from pipelines.llama2.run import (
    compile_graph,
    _generate_q_text_with_tokenizer,
    Config,
)
from pipelines.metrics.metrics import Metrics
from pipelines.nn import (
    Embedding,
    FeedForward,
    RMSNorm,
    Attention,
    Transformer,
    TransformerBlock,
)
from pipelines.weights.download import download_from_hf
from pipelines.weights.loadable_model import LlamaHParams


@always_inline
def param_key(name: String, layer_idx: Optional[Int] = None) -> String:
    """Qualify parameter name with its layer index, if passed."""
    return name + "_" + str(layer_idx.value()) if layer_idx else name


def read_hyperparams_from_dict(
    tensor_dict: TensorDict,
) -> LlamaHParams:
    dims = tensor_dict.get[DType.int32]("hyperparams.dims")[0]
    n_layers = tensor_dict.get[DType.int32]("hyperparams.n_layers")[0]
    n_heads = tensor_dict.get[DType.int32]("hyperparams.n_heads")[0]
    norm_eps = tensor_dict.get[DType.float64]("hyperparams.norm_eps")[0]
    n_kv_heads = tensor_dict.get[DType.int32]("hyperparams.n_kv_heads")[0]
    vocab_size = tensor_dict.get[DType.int32]("hyperparams.vocab_size")[0]
    return LlamaHParams(
        dims=int(dims),
        n_layers=int(n_layers),
        n_heads=int(n_heads),
        norm_eps=norm_eps,
        n_kv_heads=int(n_kv_heads),
        vocab_size=int(vocab_size),
        head_dim=int(dims // n_heads),
        n_rep=int(n_heads // n_kv_heads),
    )


struct TeenyTinyLlama[encoding: QuantizationEncoding]:
    """Builder for a teeny tiny Llama 2 model trained on TinyStories."""

    alias batch_size = 1

    var hyperparams: LlamaHParams
    """Llama 2 hyperparameters, read from the checkpoint."""

    var quantized_params: TensorDict
    """Dictionary of quantized model parameters for checkpointing."""

    def __init__(out self, model_path: Path):
        # Load quantized weights from MAX checkpoint.
        self.quantized_params = load(model_path)
        # Read Llama hyperparameters from the checkpoint.
        self.hyperparams = read_hyperparams_from_dict(self.quantized_params)

    def build(mut self) -> Graph:
        """Build the Llama 2 graph using the quantized weights from checkpoint.
        """
        # Set the KV cache and tokens input types.
        params = self.hyperparams
        cache_type = TensorType(
            DType.float32,
            "prev_seq_len",
            params.n_layers,
            Self.batch_size,
            params.n_kv_heads,
            params.head_dim,
        )
        tokens_type = TensorType(DType.int64, Self.batch_size, "seq_len")
        attn_mask_type = TensorType(DType.bool, self.batch_size, "full_seq_len")
        g = Graph(
            "TeenyTinyLlama",
            List[Type](tokens_type, attn_mask_type, cache_type, cache_type),
        )

        @parameter
        def quantize(
            name: String, layer_idx: Optional[Int] = None
        ) -> (Symbol, String):
            """Stages a quantized parameter as a constant op in the graph."""
            # Load a parameter from the MAX checkpoint.
            param = self.quantized_params.get[DType.uint8](
                param_key(name, layer_idx)
            )

            # Stage a constant op in the graph and return it with its encoding.
            return g.constant(param), Q4_0Encoding.id()

        @parameter
        def norm_weight(
            name: String, layer_idx: Optional[Int] = None
        ) -> Symbol:
            """Stages a norm weight parameter as a constant op in the graph."""
            return g.constant(
                self.quantized_params.get[DType.float32](
                    param_key(name, layer_idx)
                )
            )

        def norm(name: String, layer: Optional[Int] = None) -> RMSNorm:
            w = norm_weight(name, layer)
            return RMSNorm(params.norm_eps, w)

        layers = List[TransformerBlock[DType.float32]]()
        for layer in range(self.hyperparams.n_layers):
            # Stage a transformer block with quantized weights.
            # Read in float32 weights and quantize them before staging the op.
            # In the process, save references to the quantized tensors in the
            # parameter dictionary.
            # Doing so enables saving the checkpoint after building the graph,
            # complete with quantized weights.

            attention = Attention(
                n_heads=params.n_heads,
                n_kv_heads=params.n_kv_heads,
                head_dim=params.head_dim,
                dim=params.dims,
                use_custom_attention=True,
                wq=quantize("attn_q", layer),
                wk=quantize("attn_k", layer),
                wv=quantize("attn_v", layer),
                wo=quantize("attn_output", layer),
            )

            feed_forward = FeedForward(
                w1=quantize("ffn_gate", layer),
                w2=quantize("ffn_down", layer),
                w3=quantize("ffn_up", layer),
            )

            layers.append(
                TransformerBlock(
                    attention=attention,
                    feed_forward=feed_forward,
                    attention_norm=norm("attn_norm", layer),
                    ffn_norm=norm("ffn_norm", layer),
                )
            )

        # Stage the Llama 2 transformer model.
        embedding = Embedding(quantize("token_embd"))
        model = Transformer(
            dim=params.dims,
            n_heads=params.n_heads,
            embedding=embedding,
            layers=layers,
            norm=norm("output_norm"),
            output=quantize("token_embd"),
            theta=10000.0,
            rope_scaling=None,
        )
        outputs = model(
            tokens=g[0], attention_mask=g[1], k_cache=g[2], v_cache=g[3]
        )
        logits = outputs[0]
        g.output(List[Symbol](logits[-1, axis=1], outputs[1], outputs[2]))
        return g


def load_quantized_tinystories(checkpoint_path: Path):
    """Runs the TinyStories pipeline."""
    # Download and cache only the tokenizer config for Llama.
    tokenizer_path = download_from_hf("modularai/llama-2", "tokenizer.bin")

    # Stage the Llama model graph from the saved MAX checkpoint file.
    # This is cpu only, and thus only a BHSD layout is needed
    model = TeenyTinyLlama[Q4_0Encoding](checkpoint_path)
    graph = model.build()

    # Generate text using the quantized Llama model and the provided prompt.
    metrics = Metrics()

    config = Config()
    config.set("tokenizer-path", tokenizer_path)

    execution_device = cpu_device()
    compiled_model = compile_graph(
        graph, execution_device, config.get("custom-ops-path")[List[Path]]
    )

    params = model.hyperparams

    mojo_tokenizer = BPETokenizer.from_file(config.get("tokenizer-path")[Path])
    _generate_q_text_with_tokenizer[BPETokenizer](
        mojo_tokenizer,
        compiled_model,
        params,
        config=config,
        execution_device=execution_device,
        metrics=metrics,
    )

    print()
    metrics.print()
