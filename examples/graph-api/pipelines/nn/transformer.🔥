# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

"""Neural network layers core to transformers."""

from max.graph import ops, Symbol

from pipelines.nn import Attention, Embedding, Linear, RMSNorm


@value
struct FeedForward[WeightsT: DType = DType.float32]:
    var w1: Linear[WeightsT]
    var w2: Linear[WeightsT]
    var w3: Linear[WeightsT]

    def __call__(self, input: Symbol) -> Symbol:
        return (ops.silu(input @ self.w1) * (input @ self.w3)) @ self.w2


@value
struct TransformerBlock[WeightsT: DType = DType.float32](CollectionElement):
    var attention: Attention[WeightsT]
    var feed_forward: FeedForward[WeightsT]
    var attention_norm: RMSNorm
    var ffn_norm: RMSNorm

    def __call__(
        self,
        input: Symbol,
        start_pos: Symbol,
        freqs_cis: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> (Symbol, Symbol, Symbol):
        var attention_out: Symbol
        var k_cache_update: Symbol
        var v_cache_update: Symbol
        attention_out, k_cache_update, v_cache_update = self.attention(
            self.attention_norm(input), start_pos, freqs_cis, k_cache, v_cache
        )
        h = input + attention_out
        h = h + self.feed_forward(self.ffn_norm(h))
        return h, k_cache_update, v_cache_update


@value
struct Transformer[WeightsT: DType = DType.float32]:
    alias max_seq_len = 2048

    var dim: Int
    var n_heads: Int

    var embedding: Embedding[WeightsT]
    var layers: List[TransformerBlock[WeightsT]]
    var norm: RMSNorm
    var output: Linear[WeightsT]
    var theta: Float64

    def freqs_cis(self, start_pos: Symbol, seq_len: Symbol) -> Symbol:
        var g = start_pos.graph()
        var n = self.dim // self.n_heads
        var iota = g.range[DType.float32](0, n - 1, 2)
        var freqs = 1.0 / (self.theta ** (iota / n))
        var t = g.range[DType.float32](0, Self.max_seq_len * 2.0, 1)
        freqs = t.reshape(-1, 1) * freqs.reshape(1, -1)

        var retval = ops.stack(List(ops.cos(freqs), ops.sin(freqs)), axis=-1)
        return ops.cast(retval[start_pos : start_pos + seq_len], DType.float32)

    def __call__(
        self, tokens: Symbol, k_cache: Symbol, v_cache: Symbol
    ) -> (Symbol, Symbol, Symbol):
        start_pos = ops.shape_of(k_cache)[0]
        h = self.embedding(tokens)
        freqs_cis = self.freqs_cis(start_pos, ops.shape_of(tokens)[1])

        k_cache_updates = List[Symbol]()
        v_cache_updates = List[Symbol]()
        for i in range(len(self.layers)):
            var k_cache_layer_update: Symbol
            var v_cache_layer_update: Symbol
            h, k_cache_layer_update, v_cache_layer_update = self.layers[i](
                h, start_pos, freqs_cis, k_cache[i, axis=1], v_cache[i, axis=1]
            )
            k_cache_updates.append(k_cache_layer_update.swapaxes(0, 1))
            v_cache_updates.append(v_cache_layer_update.swapaxes(0, 1))

        return (
            self.norm(h) @ self.output,
            ops.stack(k_cache_updates, axis=1),
            ops.stack(v_cache_updates, axis=1),
        )
