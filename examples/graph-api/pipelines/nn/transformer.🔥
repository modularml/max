# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

"""Neural network layers core to transformers."""

from max.graph import ops, Symbol, Dim
from max.graph.quantization import Float32Encoding, QuantizationEncoding

from pipelines.nn import Attention, Embedding, Linear, RMSNorm

from collections import Optional


@value
struct FeedForward:
    """
    Simple FeedForward layer composed of three linear layers.
    Uses SiLU activation function.
    """

    var w1: Linear
    var w2: Linear
    var w3: Linear

    def __call__(self, input: Symbol) -> Symbol:
        return (ops.silu(input @ self.w1) * (input @ self.w3)) @ self.w2


@value
struct TransformerBlock[model_dtype: DType = DType.float32](CollectionElement):
    """
    Stacks Attention, FeedForward, and RMSNorm layers
    into single transformer block.
    """

    var attention: Attention[model_dtype]
    var feed_forward: FeedForward
    var attention_norm: RMSNorm
    var ffn_norm: RMSNorm

    def __call__(
        self,
        input: Symbol,
        freqs_cis: Symbol,
        attention_mask: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> (Symbol, Symbol, Symbol):
        attention_out, k_cache_update, v_cache_update = self.attention(
            self.attention_norm(input),
            freqs_cis,
            attention_mask,
            k_cache,
            v_cache,
        )
        h = input + attention_out
        h = h + self.feed_forward(self.ffn_norm(h))
        return h, k_cache_update, v_cache_update


@value
struct Transformer[model_dtype: DType = DType.float32]:
    """
    Transformer model consisting of TransformerBlock layers.
    """

    alias max_seq_len = 2048

    var dim: Int
    var n_heads: Int

    var embedding: Embedding
    var layers: List[TransformerBlock[model_dtype]]
    var norm: RMSNorm
    var output: Linear
    var theta: Float64
    var rope_scaling: Optional[Symbol]

    def __init__(
        mut self,
        dim: Int,
        n_heads: Int,
        embedding: Embedding,
        layers: List[TransformerBlock[model_dtype]],
        norm: RMSNorm,
        output: Linear,
        theta: Float64,
        rope_scaling: Optional[Symbol] = None,
    ):
        self.dim = dim
        self.n_heads = n_heads
        self.embedding = embedding
        self.layers = layers
        self.norm = norm
        self.output = output
        self.theta = theta
        self.rope_scaling = rope_scaling

    def freqs_cis(
        self, start_pos: Symbol, seq_len: Symbol, seq_len_dim: Dim
    ) -> Symbol:
        """
        Computes the frequency tensor for complex exponentials (cis)
        for a given seq_len. Tensor is scaled with theta parameter.
        Required to apply Rotary Position Embedding (RoPE) to tensor.
        See 'Roformer: Enhanced Transformer with Rotary Embedding'
        (arxiv.org/pdf/2104.09864).

        Args:
            start_pos: starting position of input tensor
            seq_len: length of input tensor

        Returns:
            The frequency tensor for complex exponentials
        """
        g = start_pos.graph()
        n = self.dim // self.n_heads
        iota = g.range[DType.float32](0, n - 1, 2)
        if self.rope_scaling:
            iota = iota * self.rope_scaling.value()
        freqs = 1.0 / (self.theta ** (iota / n))
        t = g.range[DType.float32](0, Self.max_seq_len * 2.0, 1)
        freqs = t.reshape(-1, 1) * freqs.reshape(1, -1)

        var retval = ops.stack(List(ops.cos(freqs), ops.sin(freqs)), axis=-1)
        return ops.cast(
            retval[
                start_pos : start_pos + seq_len, out_dims = List(seq_len_dim)
            ],
            model_dtype,
        )

    def __call__(
        self,
        tokens: Symbol,
        attention_mask: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> (Symbol, Symbol, Symbol):
        """
        Forward pass through transformer model.

        Args:
            tokens: input data represented in tokens
            attention_mask: boolean tensor indicating which elements to attend
            k_cache: cache of computed attention keys for previous tokens
            v_cache: cache of computed attention values for previous tokens

        Returns:
            output tokens,
            updated cache of computed attention keys for previous tokens,
            updated cache of computed attention values for previous tokens
        """
        start_pos = ops.shape_of(k_cache)[0]
        h = self.embedding(tokens)
        freqs_cis = self.freqs_cis(
            start_pos, ops.shape_of(tokens)[1], tokens.shape()[1]
        )

        k_cache_updates = List[Symbol]()
        v_cache_updates = List[Symbol]()
        for i in range(len(self.layers)):
            h, k_cache_layer_update, v_cache_layer_update = self.layers[i](
                h,
                freqs_cis,
                attention_mask,
                k_cache[i, axis=1, keep_dims=True],
                v_cache[i, axis=1, keep_dims=True],
            )
            k_cache_updates.append(k_cache_layer_update.swapaxes(0, 1))
            v_cache_updates.append(v_cache_layer_update.swapaxes(0, 1))

        return (
            self.norm(h) @ self.output,
            ops.stack(k_cache_updates, axis=1),
            ops.stack(v_cache_updates, axis=1),
        )
