# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""The attention mechanism used within the model."""

from math import isqrt
from max.tensor import Tensor, TensorShape

from max.graph import Graph, ops, Dim, Symbol, TensorType
from max.graph.quantization import Float32Encoding, QuantizationEncoding

from pipelines.nn import Linear


def rope(x: Symbol, freqs_cis: Symbol) -> Symbol:
    """Applies rotary positional embeddings (RoPE) to `x`.

    Args:
        x: Activation tensor with shape (batch, seq_len, n_kv_heads, head_dim).
        freqs_cis: Positional frequencies tensor with shape
            (seq_len, head_dim // 2, 2).

    Returns:
        Input activation tensor with rotary positional embeddings applied and
        the same shape as `x`.
    """
    x_complex = ops.as_interleaved_complex(x)
    x_dims = x_complex.type().tensor().dims

    freqs_cis_bcast = ops.unsqueeze(ops.unsqueeze(freqs_cis, 1), 0)

    x_re = x_complex[0, axis= -1]
    x_im = x_complex[1, axis= -1]

    freqs_re = freqs_cis_bcast[0, axis= -1].rebind(1, x_dims[1], 1, x_dims[3])
    freqs_im = freqs_cis_bcast[1, axis= -1].rebind(1, x_dims[1], 1, x_dims[3])

    rope_re = (x_re * freqs_re) - (x_im * freqs_im)
    rope_im = (x_re * freqs_im) + (x_im * freqs_re)
    rope_complex = ops.as_complex(rope_re, rope_im)

    return ops.reshape_like(rope_complex, x)


def expand_attention_mask(
    g: Graph,
    in_mask: Symbol,
    start_pos: Dim,
    seq_len: Dim,
    activation_dtype: DType,
) -> Symbol:
    """
    Given an input attention mask of shape [batch, full_seq_len], do the following:
      - Broadcast to [batch, seq_len, full_seq_len].
      - Merge with upper-triangular [seq_len, seq_len] causal mask.
    """
    # Mask out current sequence elements [i, j] where j > i with an
    # upper-triangular matrix filled with -10000.
    #
    # FIXME (KERN-782): It would be cleaner to use -INF as the min_val here, but
    # in some cases we may have a full row of padding (after second mask below).
    # In these cases, the softmax kernel currently generates a row of NANs and
    # we want to avoid that, so use -10,000 instead.
    mask_val = ops.cast(
        g.full(Scalar[DType.float32](-10000), seq_len, seq_len),
        activation_dtype,
    )
    mask = ops.band_part(
        mask_val,
        g.scalar[DType.int64](-1),
        num_upper=g.scalar[DType.int64](0),
        # Invert the mask from lower to upper.
        exclude=True,
    )

    # Compute attention scores only for the new sequence.
    # Hence for a matrix of scores of size (seqlen, cache_len + seqlen),
    # the only masked entries are (i, j) for j > cache_len + i, since row i
    # corresponds to token cache_len + i.
    extended_mask = ops.concat(
        List[Symbol](
            ops.cast(
                g.full[DType.float32](0, seq_len, start_pos),
                activation_dtype,
            ),
            mask,
        ),
        axis=1,
        out_dim=Dim("full_seq_len"),
    )

    # Reshape in_mask to [batch, 1, full_seq_len]
    in_mask = ops.unsqueeze(in_mask, 1)
    # Reshape causal mask to [1, seq_len, full_seq_len]
    extended_mask = ops.unsqueeze(extended_mask, 0)
    # FIXME (KERN-782): It would be cleaner to use -INF as the min_val here.
    min_val = ops.cast(g.scalar[DType.float32](-10000), activation_dtype)
    # Merge input & causal masks
    return ops.select(in_mask, extended_mask, min_val)


@value
struct Attention[model_dtype: DType = DType.float32]:
    var n_heads: Int
    var n_kv_heads: Int
    var head_dim: Int
    var dim: Int
    var use_custom_attention: Bool
    """Use a custom flash attention kernel if set."""

    var wq: Linear
    var wk: Linear
    var wv: Linear
    var wo: Linear

    def repeat_kv(self, kv: Symbol) -> Symbol:
        """Repeats key/value tensors to match the number of query heads."""
        batch_size = kv.shape()[0]
        kv = kv.reshape(batch_size, -1, self.n_kv_heads, 1, self.head_dim)
        kv = ops.tile(
            kv, List[Int64](1, 1, 1, self.n_heads // self.n_kv_heads, 1)
        )
        return kv.reshape(batch_size, -1, self.n_heads, self.head_dim)

    def flash_attention(
        self,
        xq: Symbol,
        xk: Symbol,
        xv: Symbol,
        attention_mask: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> Symbol:
        """Op calling into a custom "split KV cache" flash attention kernel.

        The custom flash attention kernel takes the previous KV cache and
        current KV tensors separately.
        This avoids data movement of the KV cache due to a `concat` op outside
        the flash attention kernel.
        """
        g = xq.graph()
        prev_seq_len = k_cache.shape()[0]
        seq_len = xq.shape()[1]

        attn_mask = expand_attention_mask(
            g, attention_mask, prev_seq_len, seq_len, model_dtype
        )

        # Transpose operands to their expected layouts:
        # k_cache: 1BHSD
        # v_cache: 1BHSD
        return ops.custom["with_mask_flash_attention_split_kv_cpu"](
            List(
                xq,
                xk,
                xv,
                k_cache.swapaxes(0, 1).swapaxes(1, 2).swapaxes(2, 3),
                v_cache.swapaxes(0, 1).swapaxes(1, 2).swapaxes(2, 3),
                attn_mask,
                g.constant(
                    Tensor(TensorShape(), isqrt(Float32(self.head_dim)))
                ),
            ),
            TensorType(model_dtype, 1, "seq_len", self.n_heads, self.head_dim),
        )

    def naive_attention(
        self,
        xq: Symbol,
        xk: Symbol,
        xv: Symbol,
        attention_mask: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> Symbol:
        head_dim = xq.graph().scalar(self.head_dim, model_dtype)
        prev_seq_len = k_cache.shape()[0]
        seq_len = xq.shape()[1]

        full_seq_len = Dim("full_seq_len")
        keys = ops.concat(
            List[Symbol](k_cache, xk.swapaxes(0, 1)), out_dim=full_seq_len
        ).swapaxes(0, 1)
        values = ops.concat(
            List[Symbol](v_cache, xv.swapaxes(0, 1)), out_dim=full_seq_len
        ).swapaxes(0, 1)

        # Tile keys and values if this is GQA, otherwise no-op.
        keys = self.repeat_kv(keys)
        values = self.repeat_kv(values)

        xq = xq.swapaxes(1, 2)
        keys = keys.swapaxes(1, 2)
        values = values.swapaxes(1, 2)

        scores = (xq @ keys.swapaxes(2, 3)) * ops.rsqrt(head_dim)
        expanded_mask = expand_attention_mask(
            xq.graph(), attention_mask, prev_seq_len, seq_len, model_dtype
        )
        # Reshape expanded mask to [batch, 1, seq_len, full_seq_len]
        # This implicitly broadcasts across n_heads.
        scores = scores + ops.unsqueeze(expanded_mask, 1)
        output = (ops.softmax(scores) @ values).swapaxes(1, 2)
        return output

    def attention(
        self,
        xq: Symbol,
        xk: Symbol,
        xv: Symbol,
        attention_mask: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> Symbol:
        if self.use_custom_attention:
            return self.flash_attention(
                xq, xk, xv, attention_mask, k_cache, v_cache
            )
        else:
            return self.naive_attention(
                xq,
                xk,
                xv,
                attention_mask,
                # Squeeze because naive_attention expects rank = 4 {k,v}_cache.
                ops.squeeze(k_cache, axis=1),
                ops.squeeze(v_cache, axis=1),
            )

    def __call__(
        self,
        input: Symbol,
        freqs_cis: Symbol,
        attention_mask: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> (Symbol, Symbol, Symbol):
        """Computes attention on the input, reusing the KV cache.

        Args:
            input: Activations with shape (batch, seq_len, dim).
            freqs_cis: Positional frequencies tensor with shape
                (seq_len, head_dim // 2, 2).
            attention_mask: Boolean tensor indicating which elements to attend.
            k_cache: Previously computed keys with shape
                (prev_seq_len, 1, batch, n_kv_heads, head_dim).
            v_cache: Previously computed values with shape
                (prev_seq_len, 1, batch, n_kv_heads, head_dim).

        Returns the result of multi-headed self attention on the input.
        """
        batch = input.shape()[0]
        seq_len = input.shape()[1]

        xq = input @ self.wq
        xk = input @ self.wk
        xv = input @ self.wv

        xq = xq.reshape(batch, seq_len, self.n_heads, self.head_dim)
        xk = xk.reshape(batch, seq_len, self.n_kv_heads, self.head_dim)
        xv = xv.reshape(batch, seq_len, self.n_kv_heads, self.head_dim)

        # Apply RoPE positional embeddings.
        xq = rope(xq, freqs_cis)
        xk = rope(xk, freqs_cis)

        output = self.attention(xq, xk, xv, attention_mask, k_cache, v_cache)

        output = output.reshape(batch, seq_len, -1)

        return output @ self.wo, xk, xv

    def __call__(
        self,
        input: Symbol,
        freqs_cis: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> (Symbol, Symbol, Symbol):
        """Computes attention on the input, with default attention mask."""
        g = input.graph()
        seq_len = input.shape()[1]
        prev_seq_len = k_cache.shape()[0]
        # FIXME (MSDK-765): Currently, there's no way to create a [1 x Dim+Dim]
        # node directly. Instead, we create two separate nodes and concat them
        # together as a workaround.
        attn_mask = ops.concat(
            List(g.full(True, 1, prev_seq_len), g.full(True, 1, seq_len)),
            axis=1,
            out_dim=Dim("full_seq_len"),
        )
        return self(input, freqs_cis, attn_mask, k_cache, v_cache)
