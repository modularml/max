# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

"""Neural network embedding layers."""

from max.graph import ops, Dim, StaticDim, Symbol, TensorType
from max.graph.quantization import Q4_0Encoding
from max.tensor import Tensor, TensorShape

from pipelines.nn import Linear
from pipelines.weights.ggml_quants import BlockQ40


@value
struct Embedding[WeightsT: DType = DType.float32]:
    """Quantized embedding (can be in GGML Q4_0 packed format)."""

    var weights: Symbol
    """The weights (maybe quantized Q4_0)."""

    def __call__(borrowed self, indices: Symbol) -> Symbol:
        """Gathers and dequantize rows of the quantized embedding, as needed.

        Args:
            indices: Rows of embedding to return.

        Returns:
            [Dequantized, as needed] embedding rows corresponding to `indices`.
        """

        # If there is no quantization, then just gather and return early.
        @parameter
        if WeightsT == DType.float32:
            return ops.gather(self.weights, indices, axis=0)

        g = self.weights.graph()
        quantized_tokens = ops.gather(self.weights, indices, axis=0)
        tokens_type = quantized_tokens.tensor_type()

        # Compute the dequantized output dim as the number of quantized blocks
        # times the number of elements (quants) per block.
        quantized_dim = self.weights.tensor_type().dims[-1]
        num_blocks = quantized_dim.value[StaticDim].dim // sizeof[BlockQ40]()
        alias quants_per_block = BlockQ40.QK4_0
        out_dim = int(BlockQ40.QK4_0 * num_blocks)

        # Compute shapes to reshape embeddings to matrix input expected by
        # `ggml_q4_0_dequantize`.
        final_dims = List[Dim]()
        for i in range(tokens_type.rank() - 1):
            final_dims.append(tokens_type.dim(i))
        final_dims.append(out_dim)

        tokens_shape = ops.shape_of(quantized_tokens)
        last_tokens_axis = tokens_type.rank() - 1
        reshape_shape = ops.stack(
            List(g.scalar(Int64(-1)), tokens_shape[last_tokens_axis])
        )

        dequantize_dims = List[Dim]()
        dequantize_dims.append(Dim.dynamic())
        dequantize_dims.append(tokens_type.dim(-1))

        final_shape = ops.concat(
            List(
                tokens_shape[:last_tokens_axis],
                g.constant[DType.int64](
                    Tensor[DType.int64](TensorShape(1), out_dim)
                ),
            )
        )

        # Reshape gathered token embeddings to a matrix expected by dequantize.
        reshaped_tokens = ops.reshape(
            quantized_tokens, reshape_shape, dequantize_dims
        )

        # Dequantize to floating point.
        dequantized_tokens = ops.custom["ggml_q4_0_dequantize"](
            reshaped_tokens,
            TensorType(DType.float32, Dim.dynamic(), Dim.dynamic()),
        )

        # Restore original `rank(quantized_tokens) - 1` gather dimensions.
        return ops.reshape(dequantized_tokens, final_shape, final_dims)
