[project]
authors = ["Modular <hello@modular.com>"]
channels = ["conda-forge", "https://conda.modular.com/max/"]
description = "End-to-end execution of Llama3 using the Max Engine"
name = "Python Pipelines"
platforms = ["osx-arm64", "linux-aarch64", "linux-64"]
version = "0.1.0"

[tasks]
llama3 = "python pipelines.py llama3"
replit = "python pipelines.py replit"
mistral = "python pipelines.py mistral"
serve = "python pipelines.py serve"

[dependencies]
python = ">=3.9,<3.13"
max = ">=24.6.0.dev2024090821"

[pypi-dependencies]
click = ">=8.1.7"
gguf = ">=0.10.0"
requests = ">=2.32.3"
sentencepiece = ">=0.2.0"
tokenizers = ">=0.19.1"
transformers = ">=4.44.2"
psutil = ">=6.0.0"
pyinstrument  = ">=4.7.3"
prometheus-client = ">=0.20.0"
huggingface_hub = ">=0.26.2"
scipy = "==1.13.1"

# PyTorch 2.4.1 with cuda support
torch = "==2.4.1"
torchvision = "==0.19.1"
torchaudio = "==2.4.1"

# Dependencies of `max`, not available through conda-forge
prometheus-async = ">=22.2.0"
faster-fifo = "==1.4.7"
